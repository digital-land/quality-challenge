{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Document URLs\n",
    "\n",
    "Every entity should have an associated document which is the *legal instrument* that defines the entity. These should be stored under the `document_url` field of the entity. This notebook uses the webcrawler and search functionality included in the repository to find the correct document url for a sample of conservation areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import requests\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "from data_quality_utils import Crawler\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 1. Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_df = pl.read_csv(\"data/test_data.csv\")\n",
    "DATA_FILE = \"datasette_data.csv\"\n",
    "QUERY_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_DATA:\n",
    "    # get data from datasette\n",
    "    datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    select \n",
    "    l.entity,\n",
    "    o.website,\n",
    "    o.organisation \n",
    "    from lookup as l\n",
    "    left join organisation as o\n",
    "    on l.organisation=o.organisation \n",
    "    where l.entity in {}\n",
    "    and o.website != 'https://historicengland.org.uk'\n",
    "    \"\"\".format(\n",
    "        tuple(test_cases_df[\"entity\"].to_list())\n",
    "    )\n",
    "    encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "    r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=(\"user\", \"pass\"))\n",
    "\n",
    "    with open(DATA_FILE, \"wb\") as f_out:\n",
    "        f_out.write(r.content)\n",
    "data = pl.read_csv(DATA_FILE)\n",
    "test_cases_df = test_cases_df.join(data, on=\"entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text_from_url(urls):\n",
    "    pdf_embeddings = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        pdf_filename = \"pdf_temp_file.pdf\"\n",
    "        with open(pdf_filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        pdf_text = extract_text(pdf_filename)\n",
    "        pdf_embedding = embedding_model.encode(pdf_text)\n",
    "        pdf_embeddings.append(pdf_embedding)\n",
    "    return np.array(pdf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(sorted_df, num_results):\n",
    "    # print top n urls with similarity scores\n",
    "    print(\"\\nTop Similar PDFs:\\n\" + \"=\" * 40)\n",
    "    for i in range(min(num_results, len(sorted_df))):\n",
    "        url = sorted_df.get_column(\"url\")[i]\n",
    "        score = sorted_df.get_column(\"similarity\")[i]\n",
    "        print(f\"{i+1}. {url.ljust(60)} | Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Our approach\n",
    "\n",
    "Our approach involves 2 main steps: a web crawler and an embedding similarity search. Below is a description of these steps.\n",
    "\n",
    "### Web crawler\n",
    "\n",
    "The web crawler takes a homepage URL of an organisation (council website) and crawls it to look for pages talking about conservation areas.\n",
    "\n",
    "The crawler will look for links on a single page, put them in a queue and then iteratively check them until it finds what it was looking for or it reaches a stopping criterion, such as maximum depth (how many clicks away from home page). \n",
    "\n",
    "In order to save time, we can define some scorers or filters which tell the crawler which pages to prioritise or ignore. In this case, some common patterns of what a user needs to click to get to the page of interest are _\"planning\"_, _\"building\"_, _\"heritage\"_ or _\"conservation\"_.\n",
    "\n",
    "The crawler uses a *\"best first strategy\"*, which utilises the scorers or filters to visit most relevant sites first, rather than a depth-first or breath-first search.\n",
    "\n",
    "The crawler extracts the HTML from the pages and turns them into markdown. This is because it's more readable and easier to work with in the next steps. The crawler returns a list of pairs of (_url_, _markdown_).\n",
    "\n",
    "### Embedding search\n",
    "\n",
    "To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Gedling - full example retrieving similar PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all PDFs at the URL patterns, extracts text and uses embedding similarity to search for best matches\n",
    "max_depth = 6\n",
    "num_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\", \"*heritage*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = \"\"\"\n",
    "The text describes a conservation area. It includes information about\n",
    "where the conservation area is, its history, archeology, boundaries\n",
    "and any additional planning data. Usually it includes images or maps\n",
    "showing the boundary of the conservation area.\n",
    "\"\"\"\n",
    "\n",
    "await process_council(\n",
    "    council_names=[\"Gedling\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    crawl_type=\"pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Retrieving All Test PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_council_pdfs(\n",
    "    council_names,\n",
    "    max_depth=6,\n",
    "    keyword_scorer=None,\n",
    "    filters=None,\n",
    "    cache_enabled=False,\n",
    "    crawl_type=\"html\",\n",
    "):\n",
    "    crawler = Crawler(\n",
    "        max_depth=max_depth,\n",
    "        keyword_scorer=keyword_scorer,\n",
    "        filters=filters,\n",
    "        cache_enabled=cache_enabled,\n",
    "        crawl_type=crawl_type,\n",
    "    )\n",
    "\n",
    "    for council_name in council_names:\n",
    "        council_data = data.filter(pl.col(\"name\").str.contains(council_name))\n",
    "        full_name = council_data.get_column(\"name\")[0]\n",
    "        homepage = council_data.get_column(\"website\")[0]\n",
    "        print(\"=\" * 40 + f\"\\nCrawling {full_name}...\\n\")\n",
    "\n",
    "        # crawl url\n",
    "        crawl_data = await crawler.deep_crawl(homepage)\n",
    "\n",
    "        for url in crawl_data:\n",
    "            print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only searches for all PDFs at the URL patterns\n",
    "max_depth = 6\n",
    "num_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\", \"*heritage*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "await get_council_pdfs(\n",
    "    council_names=[\"South Gloucestershire\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    "    crawl_type=\"pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Bournemouth, Christchurch and Poole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only searches for all PDFs at the URL patterns\n",
    "max_depth = 6\n",
    "num_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\", \"*heritage*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "await get_council_pdfs(\n",
    "    council_names=[\"Bournemouth, Christchurch and Poole\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    "    crawl_type=\"pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Warrington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only searches for all PDFs at the URL patterns\n",
    "max_depth = 6\n",
    "num_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\", \"*heritage*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "await get_council_pdfs(\n",
    "    council_names=[\"Warrington\"], max_depth=max_depth, filters=filters, crawl_type=\"pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Stoke on Trent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only searches for all PDFs at the URL patterns\n",
    "max_depth = 6\n",
    "num_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\", \"*heritage*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "await get_council_pdfs(\n",
    "    council_names=[\"Stoke\"], max_depth=max_depth, filters=filters, crawl_type=\"pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhclg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
