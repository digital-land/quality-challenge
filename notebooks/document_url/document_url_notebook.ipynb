{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Document URLs\n",
    "\n",
    "Every entity should have an associated document which is the *legal instrument* that defines the entity. These should be stored under the `document_url` field of the entity. This notebook uses the webcrawler and search functionality included in the repository to find the correct document url for a sample of conservation areas.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "This notebook assumes that all documentation_urls have been validated when the document_url search begins. This allows us to do a shallow crawl from the documentation_url of the entity rather than a deep crawl from the homepage.\n",
    "\n",
    "The reason this is important is that PDFs are typically stored in a generic file folder (eg. www.council-name.gov.uk/files/all_the_pdfs.pdf) so we can't use keyword filters on the URL as we did in the documentation_url notebook. Without those filters, the entire website is crawled and a large amount of files are returned. This makes the notebook take a very long time and require a lot of memory to run. Whereas shallow crawls from validation documentation_urls successfully result in finding the document_url in a much shorter time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "from data_quality_utils.crawler import Crawler\n",
    "from data_quality_utils.crawler.utils import clean_url\n",
    "from data_quality_utils.similarity_searcher import SimilaritySearcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Sample\n",
    "\n",
    "To test the approach, we include a small set of conservation areas with known `document_urls` in this repository. In this section, we load that data and then query datasette to find the organisation responsible for each. This will give us the key details we need to scrape the organisations' websites and find the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_df = pl.read_csv(\"data/test_data.csv\")\n",
    "DATA_FILE = \"datasette_data.csv\"\n",
    "QUERY_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_DATA:\n",
    "    # get data from datasette\n",
    "    datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    select \n",
    "    l.entity,\n",
    "    o.website,\n",
    "    o.organisation \n",
    "    from lookup as l\n",
    "    left join organisation as o\n",
    "    on l.organisation=o.organisation \n",
    "    where l.entity in {}\n",
    "    and o.website != 'https://historicengland.org.uk'\n",
    "    \"\"\".format(\n",
    "        tuple(test_cases_df[\"entity\"].to_list())\n",
    "    )\n",
    "    encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "    r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=(\"user\", \"pass\"))\n",
    "\n",
    "    with open(DATA_FILE, \"wb\") as f_out:\n",
    "        f_out.write(r.content)\n",
    "data = pl.read_csv(DATA_FILE)\n",
    "test_cases_df = test_cases_df.join(data, on=\"entity\").unique()\n",
    "test_cases_df = test_cases_df.with_columns(\n",
    "    document_url=test_cases_df[\"document_url\"].str.replace_all(\"%20\", \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Scrape Websites\n",
    "\n",
    "The web crawler was introduced for the `documentation_url` challenge. It takes the homepage URL of an organisation (council website) and crawls it to look for pages given some keyword filters to ensure that it does not scrape the entire website including many pages that are not of interest.\n",
    "\n",
    "We have extended the crawler to return the URLs of PDFs when the `crawl_type` parameter is set to `pdf`. We will use this setting to scrape all PDFs from the websites in the test data.\n",
    "\n",
    "### Example PDF Scraping\n",
    "As a fast example, we show that a max_depth of 1 starting from the correct `documentation_url` is quick and returns the right PDF for Napsbury. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only searches for all PDFs at the URL patterns\n",
    "max_depth = 1\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\", \"application/pdf\"]},\n",
    "]\n",
    "\n",
    "crawler = Crawler(\n",
    "    max_depth=max_depth,\n",
    "    keyword_scorer=None,\n",
    "    filters=filters,\n",
    "    cache_enabled=False,\n",
    "    crawl_type=\"pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_data = await crawler.deep_crawl(\"https://www.stalbans.gov.uk/conservation-areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "napsbury_document_url = test_cases_df.filter(test_cases_df[\"name\"] == \"Napsbury\")[\n",
    "    \"document_url\"\n",
    "][0]\n",
    "napsbury_document_url in crawl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Scrape all test cases\n",
    "Next we use this functionality to get all URLs of all test cases. For now, set the max_depth to 6 but ideally you would have aleady validated the `documentation_url` for all entities and can use a max_depth of one or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls = dict()\n",
    "\n",
    "for documentation_url in test_cases_df[\"documentation_url\"]:\n",
    "    if documentation_url not in pdf_urls:\n",
    "        print(40 * \"*\")\n",
    "        print(f\"Starting {documentation_url}\")\n",
    "        crawl_data = await crawler.deep_crawl(documentation_url)\n",
    "        pdf_urls[documentation_url] = crawl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Often multiple links with and without www. or with different conventions on how to display spaces in the file name are recovered. Deduplicate by fixing these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for website, url_list in pdf_urls.items():\n",
    "    if url_list:\n",
    "        pdf_urls[website] = list(map(clean_url,url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Store these to prevent re-scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pdf_urls.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pdf_urls, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 2. Markitdown\n",
    "We use the package markitdown to convert our PDF URLs to markdown text. This will allow us to perform a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pdf_urls.pickle\", \"rb\") as f:\n",
    "    pdf_urls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf(pdf_url):\n",
    "    try:\n",
    "        md = MarkItDown(enable_plugins=False)\n",
    "        text = md.convert(pdf_url).markdown\n",
    "        return text\n",
    "    except:\n",
    "        return \"Fail.\"\n",
    "\n",
    "\n",
    "def document_df_from_urls(website, url_list):\n",
    "    df = pl.DataFrame(data=url_list, schema=[\"id\"])\n",
    "    df = df.with_columns(text=df[\"id\"].map_elements(convert_pdf, return_dtype=str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "This DataFrame with every document represented by an ID (here the url for simplicity) and having associated markdown text is the format expected by our similarity search function so we will process all test cases in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dfs = dict()\n",
    "for website, url_list in pdf_urls.items():\n",
    "    print(website)\n",
    "    if website not in pdf_dfs:\n",
    "        pdf_dfs[website] = document_df_from_urls(website=website, url_list=url_list)\n",
    "        with open(\"data/pdf_dfs.pickle\", \"wb\") as f:\n",
    "            pickle.dump(pdf_dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 3. Search\n",
    "\n",
    "Finally we use the similarity searcher to find the most similar document to a query that represents the text we'd expect to find in the entity definition document. We'll make the assumption that the name of the conservation area will appear in its document to greatly simplify this process. `SimilaritySearcher` embeds text after removing all irrelevant documents so the use of keywords with remove the vast majority of PDFs and make this much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pdf_dfs.pickle\", \"rb\") as f:\n",
    "    pdf_dfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in pdf_dfs.items():\n",
    "    df = df.with_columns(id=df[\"id\"].map_elements(clean_url, return_dtype=str))\n",
    "    pdf_dfs[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(sorted_df, num_results):\n",
    "    # print top n urls with similarity scores\n",
    "    print(\"\\nTop Similar PDFs:\\n\" + \"=\" * 40)\n",
    "    for i in range(min(num_results, len(sorted_df))):\n",
    "        url = sorted_df.get_column(\"id\")[i]\n",
    "        score = sorted_df.get_column(\"similarity\")[i]\n",
    "        print(f\"{i+1}. {url.ljust(60)} | Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Section 69 of the Planning (Listed Buildings and Conservation Areas) Act 1990 states that \n",
    "every local authority shall determine areas of spcecial architectural or historic interest and \n",
    "designate them as conservation areas.\n",
    "\n",
    "The aims of this Character Statement are to show the way in which the form of the\n",
    "conservation area has evolved and to assess its present character; to indicate the principles\n",
    "to be adopted in considering planning applications in the area; and to form a framework\n",
    "within which more detailed proposals may be formulated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SimilaritySearcher(strategy=\"best_of_three\")\n",
    "results = dict()\n",
    "for (\n",
    "    entity_id,\n",
    "    entity_name,\n",
    "    correct_url,\n",
    "    documentation_url,\n",
    "    _,\n",
    "    _,\n",
    ") in test_cases_df.iter_rows():\n",
    "    document_df = pdf_dfs[documentation_url]\n",
    "    results[entity_name] = searcher.search(\n",
    "        query=query, document_df=document_df, keyword_filters=[entity_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_results(results[\"Napsbury\"], num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list()\n",
    "num_unclassified = 0\n",
    "for (\n",
    "    entity_id,\n",
    "    entity_name,\n",
    "    correct_url,\n",
    "    documentation_url,\n",
    "    _,\n",
    "    _,\n",
    ") in test_cases_df.iter_rows():\n",
    "    if not results[entity_name][\"id\"].is_empty():\n",
    "        rank = results[entity_name][\"id\"].index_of(clean_url(correct_url))\n",
    "        if rank is not None:\n",
    "            ranks.append(rank + 1)\n",
    "            continue\n",
    "        else:\n",
    "            num_unclassified += 1\n",
    "    else:\n",
    "        num_unclassified += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3.5, 4), tight_layout=True)\n",
    "ax = sns.histplot(\n",
    "    x=ranks + num_unclassified * [10],\n",
    "    binwidth=1,\n",
    "    discrete=True,\n",
    "    color=\"#00625E\",\n",
    "    stat=\"proportion\",\n",
    ")\n",
    "ax.set_title(f\"Search Ranking of Correct Document\")\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.set_xticklabels([f\"{i}\" for i in range(1, 8)] + [\"\", \"\", \"Not Found\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhclg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
