{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Documentation URL Search\n",
    "\n",
    "In this notebook, we demonstrate how the tools in this repository could be used to validate and search for documentation urls for entities in the planning.data.gov.uk data.\n",
    "\n",
    "The approach involves 2 main steps: a web crawler and an embedding similarity search. The webcrawler finds all potentially relevant pages of a council's website using very simple filters and the similarity search then finds the pages most relevant to a user input query. By making this input reflective of the expected content of the page the documentation_url would point to, we should find some strong candiate pages.\n",
    "\n",
    "This notebook demonstrates the two tools (crawler and search) and runs the search for a small number of test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from statistics import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "from data_quality_utils.crawler import Crawler\n",
    "from data_quality_utils.similarity_searcher import (\n",
    "    SimilaritySearcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from datasette\n",
    "datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "query = \"\"\"\n",
    "select *\n",
    "from source as s\n",
    "left join organisation as o\n",
    "on s.organisation=o.organisation\n",
    "where s.collection=\"conservation-area\"\n",
    "\"\"\"\n",
    "encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=(\"user\", \"pass\"))\n",
    "\n",
    "filename = \"data/datasette_data.csv\"\n",
    "with open(filename, \"wb\") as f_out:\n",
    "    f_out.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by organisation as we're looking for one page per council\n",
    "filename = \"data/datasette_data.csv\"\n",
    "\n",
    "council_data = (\n",
    "    pl.read_csv(filename)\n",
    "    .group_by(\"name\")\n",
    "    .agg(pl.col(\"website\").first(), pl.col(\"documentation_url\"))\n",
    ")\n",
    "council_data = council_data.with_columns(pl.col(\"website\").str.strip_chars_end(\"/\"))\n",
    "council_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Web crawler\n",
    "\n",
    "The web crawler takes a homepage URL of an organisation (council website) and crawls it to look for pages talking about conservation areas.\n",
    "\n",
    "The crawler will look for links on a single page, put them in a queue and then iteratively check them until it finds what it was looking for or it reaches a stopping criterion, such as maximum depth (how many clicks away from home page). \n",
    "\n",
    "In order to save time, we can define some scorers or filters which tell the crawler which pages to prioritise or ignore. In this case, some common patterns of what a user needs to click to get to the page of interest are _\"planning\"_, _\"building\"_, _\"heritage\"_ or _\"conservation\"_.\n",
    "\n",
    "The crawler uses a *\"best first strategy\"*, which utilises the scorers or filters to visit most relevant sites first, rather than a depth-first or breath-first search.\n",
    "\n",
    "The crawler extracts the HTML from the pages and turns them into markdown. This is because it's more readable and easier to work with in the next steps. The crawler returns a list of pairs of (_url_, _markdown_).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "council = council_data[0, \"name\"]\n",
    "homepage = council_data[0, \"website\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(\n",
    "    max_depth=1,  # clicks from home page\n",
    "    cache_enabled=False,\n",
    ")\n",
    "print(f\"Crawling {council}\")\n",
    "crawl_data = await crawler.deep_crawl(homepage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 1.2 Filters and scorers\n",
    "\n",
    "In the previous example, the depth the crawler searched was kept low to increase the speed of crawling with no other settings. However, higher depths are required to find most documentation_urls so we show here how to use filters and keyword scorers to restrict the time taken and number of pages returned whilst still ensuring all relevant pages are collected.\n",
    "\n",
    "These filters and scorers are one of the key things to change when trying to search for new entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_scorer = {\n",
    "    \"keywords\": [\n",
    "        \"conservation\",\n",
    "        \"conservation area\",\n",
    "        \"planning\",\n",
    "        \"building\",\n",
    "        \"urban\",\n",
    "        \"heritage\",\n",
    "        \"resident\",\n",
    "    ],\n",
    "    \"weight\": 0.8,\n",
    "}\n",
    "\n",
    "filters = [\n",
    "    {\n",
    "        \"type\": \"SEOFilter\",\n",
    "        \"threshold\": 0.6,\n",
    "        \"keywords\": [\"conservation\", \"area\", \"planning\", \"heritage\", \"resident\"],\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"ContentRelevanceFilter\",\n",
    "        \"query\": \"conservation area or planning data\",\n",
    "        \"threshold\": 0.2,\n",
    "    },\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "council = council_data[1, \"name\"]\n",
    "homepage = council_data[1, \"website\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(\n",
    "    max_depth=4,  # clicks from home page\n",
    "    cache_enabled=False,\n",
    "    keyword_scorer=keyword_scorer,\n",
    "    filters=filters,\n",
    ")\n",
    "print(f\"Crawling {council}\")\n",
    "crawl_data = await crawler.deep_crawl(homepage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 1.3 Downloading the test set\n",
    "\n",
    "In order to test the search functionality, we have collected 25 correct documentation URLs and included them in this repository. In this section, we scrape the relevant websites for those test cases and store them. This can take several hours but since each site is saved as it is scraped, you only need to run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_council(\n",
    "    council_name,\n",
    "    council_website,\n",
    "    max_depth=6,\n",
    "    keyword_scorer=None,\n",
    "    filters=None,\n",
    "    cache_enabled=False,\n",
    "    save_dfs=True,\n",
    "    load_dfs=True,\n",
    "    data_dir=\"\",\n",
    "):\n",
    "    crawler = Crawler(\n",
    "        max_depth=max_depth,\n",
    "        keyword_scorer=keyword_scorer,\n",
    "        filters=filters,\n",
    "        cache_enabled=cache_enabled,\n",
    "    )\n",
    "\n",
    "    short_council_name = council_website.split(\".\")[1]\n",
    "    save_path = f\"{data_dir}{short_council_name}.csv\"\n",
    "\n",
    "    print(\"=\" * 40 + f\"\\nProcessing {council_name}...\\n\")\n",
    "\n",
    "    if os.path.isdir(data_dir) and load_dfs:\n",
    "        if f\"{short_council_name}.csv\" in os.listdir(data_dir):\n",
    "            crawl_df = pl.read_csv(save_path)\n",
    "            return crawl_df\n",
    "\n",
    "    # crawl url\n",
    "    crawl_data = await crawler.deep_crawl(council_website)\n",
    "\n",
    "    crawl_df = pl.DataFrame(crawl_data, schema=[\"id\", \"text\"], orient=\"row\")\n",
    "\n",
    "    if save_dfs:\n",
    "        crawl_df.write_csv(save_path)\n",
    "\n",
    "    return crawl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_csv(\"data/page_ranking_truth_df.csv\")\n",
    "test_df = test_df.with_columns(\n",
    "    pl.col(\"correct_documentation_url\").str.strip_chars_end(\"/\")\n",
    ")\n",
    "search_tests = council_data.join(test_df, on=\"website\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = dict()\n",
    "\n",
    "for council_name, council_website, _, _ in search_tests.iter_rows():\n",
    "    scraped_data[council_website] = await scrape_council(\n",
    "        council_name=council_name,\n",
    "        council_website=council_website,\n",
    "        keyword_scorer=None,\n",
    "        filters=filters,\n",
    "        save_dfs=True,\n",
    "        load_dfs=True,\n",
    "        data_dir=\"data/crawled_sites/\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 2. Embedding search\n",
    "\n",
    "Embedding is a method where a vector representation of our scraped markdown text is generated. The way the embedding model is trained ensures that that vector numerically captures the meaning of that text. As a result, we can then measure the closeness of the meaning of two pieces of text using the cosine similarity of their embedding vectors - a standard mathematical approach to measuring the similarity of vectors.\n",
    "\n",
    "Our goal is to find the webpage with the highest cosine similarity to our example prompt, which can be user specified for the type of thing we are searching for, here it identifies conservation areas but in principle could be changed for article 4 directions or similar.\n",
    "\n",
    "At present we have three strategies: \n",
    "1) Embed the entire webpage.\n",
    "2) Chunk the webpage into smaller texts (\"chunks\") and find the page with the best matching chunk\n",
    "3) Chunk and find the webpage where the three most similar chunks to the query have the highest average\n",
    "\n",
    "We recommend the third approach - using chunks gives a better range of similarities from bad matches to strong matches as the meaning of key bits of a page are not washed out by averaging over teh whole page. However, using a few chunks rather than the single best prevents matches to pages that are simply a link to the page you want with a short paragraph description.\n",
    "\n",
    "If the latter is chosen, we trying to find the webpage <i>that <b>has</b> the chunk</i> with the highest embedding similarity. The parameter `chunk_size` determines the approximate size of these chunks, split at one of the `separators`. There is also `chunk_overlap` to specify how much of the previous chunk you want to begin the next, which is useful for preserving context.\n",
    "\n",
    "To limit the quantity of these matches returned, specify the cutoff, organised by similarity with `num_results`, and specify how many of these are printed after the crawler and embedding model has run with `num_printing_results`. These scores can be saved and loaded in `data_dir` using `save_dfs` and `load_dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Conservation Areas in {}\n",
    "\n",
    "We are committed to preserving the historic and architectural character of our borough\n",
    "through designated conservation areas. These areas protect buildings, streets, and \n",
    "landscapes of special significance, ensuring that any changes respect their unique heritage. \n",
    "If you live or own property in a conservation area, additional planning controls may \n",
    "apply to alterations, demolitions, and new developments. Our aim is to balance modern \n",
    "needs with the protection of our historic environment. For more information on conservation \n",
    "area guidelines, planning applications, and how you can contribute to local heritage preservation, \n",
    "please visit our planning and conservation pages. You will find maps, appraisal documents and the\n",
    "list of council conservation areas. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(sorted_df, num_results):\n",
    "    \"\"\"\n",
    "    Print top n URLs with their similarity score.\n",
    "    Assumes df is sorted.\n",
    "    \"\"\"\n",
    "    print(\"\\nTop Similar Pages:\\n\" + \"=\" * 40)\n",
    "    for i in range(min(num_printing_results, len(sorted_df))):\n",
    "        url = sorted_df.get_column(\"id\")[i]\n",
    "        score = sorted_df.get_column(\"similarity\")[i]\n",
    "        print(f\"{i+1}. {url.ljust(60)} | Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SimilaritySearcher(strategy=\"document\")\n",
    "prompt = prompt_template.format(\"Wirral\")\n",
    "res = searcher.search(prompt, scraped_data[\"https://www.wirral.gov.uk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_results(res, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Evaluating Results with Chunking\n",
    "\n",
    "For this section we manually labelled 25 councils with true page of their conservation area list. We perform tests and ranking here to assess the performance of our model. First we import a manually defined list of true documentation urls as test_df, and filter our main dataset for that. We also strip the final slash and will clean urls in other ways throughout.\n",
    "\n",
    "Embedding all of the pages from all of the councils can take a while. We improve this by using keyword filters to remove pages that don't at least mention a concept that we expect to see on the page we are searching for. These should be used sparingly, really only when you are certain the correct page will contain a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGY = \"best_of_three\"\n",
    "searcher = SimilaritySearcher(strategy=STRATEGY)\n",
    "\n",
    "results = dict()\n",
    "for name, website, _, correct_url in search_tests.iter_rows():\n",
    "    # Scraper collects page sections as separate pages so drop duplicates\n",
    "    # eg. main.html#main and main.html are considered different pages\n",
    "    document_df = scraped_data[website].unique(subset=[\"text\"])\n",
    "\n",
    "    # clean URLs for matching\n",
    "    document_df = document_df.with_columns(\n",
    "        id=document_df[\"id\"].map_elements(lambda x: x.split(\"#\")[0].strip(\"/\"))\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(name)\n",
    "    results[website] = searcher.search(\n",
    "        query=prompt, document_df=document_df, keyword_filters=[\"conservation\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list()\n",
    "num_unclassified = 0\n",
    "\n",
    "for _, website, _, correct_url in search_tests.iter_rows():\n",
    "    rank = results[website][\"id\"].index_of(correct_url)\n",
    "    if rank is not None:\n",
    "        ranks.append(rank + 1)\n",
    "    else:\n",
    "        num_unclassified += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = ranks.count(mode(ranks))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax = sns.histplot(\n",
    "    x=ranks, binwidth=1, discrete=True, color=\"#00625E\", stat=\"proportion\"\n",
    ")\n",
    "ax.set_title(f\"Frequency of Ranks - {num_unclassified} unclassified\")\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(range(1, 25))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(ranks):\n",
    "    size = len(ranks)\n",
    "    return sum([1 / rank for rank in ranks]) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = mean_reciprocal_rank(ranks)\n",
    "print(f\"Reciprocal of Mean Reciprocal Rank: {1/mrr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Case Studies\n",
    "\n",
    "Let's look a little further at the ones that did not work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_page = search_tests.filter(\n",
    "    search_tests[\"website\"] == \"https://www.camden.gov.uk\"\n",
    ")[\"correct_documentation_url\"][0]\n",
    "print(\n",
    "    scraped_data[\"https://www.camden.gov.uk\"].filter(pl.col(\"id\") == correct_page)[\n",
    "        \"text\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "For Dover, the web scraper simply does not parse the website properly. It only crawls over four pages, the use of ASPX file types is breaking the crawler even though it is ultimately HTML. Crawl4AI only checks the file type not the content when deciding what to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_page = search_tests.filter(\n",
    "    search_tests[\"website\"] == \"https://www.dover.gov.uk\"\n",
    ")[\"correct_documentation_url\"][0]\n",
    "print(\n",
    "    scraped_data[\"https://www.dover.gov.uk\"].filter(pl.col(\"id\") == correct_page)[\n",
    "        \"text\"\n",
    "    ]\n",
    ")\n",
    "print(len(scraped_data[\"https://www.dover.gov.uk\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "This may be down to the .aspx extension used for the website, so the web crawler may not be appropriate in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhclg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
