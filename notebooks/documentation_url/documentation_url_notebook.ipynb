{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4361c7b-e149-475f-823b-e56baa3d3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from data_quality_utils import Crawler\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Optional, Union\n",
    "from statistics import mode\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
    "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, BestFirstCrawlingStrategy\n",
    "from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n",
    "from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n",
    "from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter, ContentRelevanceFilter, SEOFilter, ContentTypeFilter\n",
    "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
    "from crawl4ai.content_filter_strategy import BM25ContentFilter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701c33a-ed1e-4b0e-a6c4-a3ba361a4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# suppress warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7e124-377a-411f-9507-5e2e5296ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from datasette\n",
    "datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "query = \"\"\"\n",
    "select * \n",
    "from source as s\n",
    "left join organisation as o\n",
    "on s.organisation=o.organisation \n",
    "where s.collection = \"conservation-area\"\n",
    "\"\"\"\n",
    "encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=('user', 'pass'))\n",
    "\n",
    "filename = \"datasette_data.csv\"\n",
    "with open(filename, \"wb\") as f_out:\n",
    "    f_out.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff92ca-01a5-4616-a301-3f8f98cd4020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group by organisation as we're looking for one page per council\n",
    "data = (\n",
    "    pl.read_csv(filename)\n",
    "    .group_by(\"name\")\n",
    "    .agg(pl.col(\"website\").first(), pl.col(\"documentation_url\"))\n",
    ")\n",
    "data = data.with_columns(pl.col(\"website\").str.strip_chars_end(\"/\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size, chunk_overlap, separators, keep_separator):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,  \n",
    "        separators=separators,\n",
    "        keep_separator=keep_separator\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_processing(crawl_data, chunk_size, chunk_overlap, separators, keep_separator):\n",
    "    chunking_data = list()\n",
    "    \n",
    "    for url, markdown in crawl_data:\n",
    "        chunk_texts = split_text(markdown, chunk_size, chunk_overlap, separators, keep_separator)\n",
    "        for chunk_num, chunk in enumerate(chunk_texts):\n",
    "            chunk_id = f\"{url}-{chunk_num}\" # To have unique id for data.\n",
    "            chunking_data.append((chunk_id, url, chunk_num, chunk))\n",
    "    \n",
    "    chunked_df = pl.DataFrame(chunking_data, \n",
    "                              schema=[\"id\", \"url\", \"chunk_num\", \"text\"], orient=\"row\")\n",
    "    \n",
    "    embeddings = embedding_model.encode(chunked_df['text'].to_list(), convert_to_numpy=True)\n",
    "    chunked_df = chunked_df.with_columns(pl.Series(name=\"embedding\", values=embeddings))\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_processing(crawl_data):\n",
    "    basic_data = list()\n",
    "    \n",
    "    for url, markdown in crawl_data: \n",
    "        basic_data.append((f\"{url}-0\", url, 0, markdown))\n",
    "    \n",
    "    basic_df = pl.DataFrame(basic_data, \n",
    "                            schema=[\"id\", \"url\", \"chunk_num\", \"text\"], orient=\"row\")\n",
    "    \n",
    "    embeddings = embedding_model.encode(basic_df['text'].to_list(), convert_to_numpy=True)\n",
    "    basic_df = basic_df.with_columns(pl.Series(name=\"embedding\", values=embeddings))\n",
    "\n",
    "    return basic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f3be6-d1c0-454a-8e4a-2be416a523eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_urls(\n",
    "        crawl_data, \n",
    "        prompt, \n",
    "        num_results, \n",
    "        chunking,\n",
    "        chunk_size, \n",
    "        chunk_overlap, \n",
    "        separators, \n",
    "        keep_separator,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Embeds crawled webpage data, computes similarity to a given prompt, and returns the top N most similar pages.\n",
    "\n",
    "    Parameters:\n",
    "    - crawl_data (list of tuples): A list of tuples containing (url, markdown) for each crawled page.\n",
    "    - prompt (str): The text prompt to compare against the crawled page embeddings.\n",
    "    - num_results (int, optional): The number of top similar pages to return. If None, returns all pages.\n",
    "    - chunking (bool, optional): Whether we want to embed the full webpage or chunk.\n",
    "\n",
    "    Returns:\n",
    "    - polars.DataFrame: A DataFrame containing:\n",
    "        - \"url\": The webpage URL.\n",
    "        - \"markdown\": The extracted markdown content.\n",
    "        - \"embedding\": The computed embedding for the content.\n",
    "        - \"similarity\": The cosine similarity score with the prompt.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    if chunking:\n",
    "        crawl_df = chunk_processing(\n",
    "            crawl_data = crawl_data,\n",
    "            chunk_size = chunk_size, \n",
    "            chunk_overlap = chunk_overlap, \n",
    "            separators = separators, \n",
    "            keep_separator = keep_separator\n",
    "        )\n",
    "    elif not chunking:\n",
    "        crawl_df = basic_processing(crawl_data)\n",
    "    \n",
    "    embeddings = np.stack(crawl_df[\"embedding\"].to_list())\n",
    "    \n",
    "    prompt_embedding = np.array(embedding_model.encode(prompt, convert_to_numpy=True), dtype='float64')\n",
    "    \n",
    "    # get similarity scores\n",
    "    sim=util.cos_sim(\n",
    "        prompt_embedding.astype(np.float32), \n",
    "        embeddings.astype(np.float32)\n",
    "    )\n",
    "    # get indices of top n most similar urls\n",
    "    if not num_results:\n",
    "        num_results = len(crawl_df)\n",
    "    indices = np.argsort(sim).numpy().flatten()[:-num_results-1:-1]\n",
    "    sorted_df = (\n",
    "        crawl_df[indices]\n",
    "        .with_columns(similarity=np.sort(sim).flatten()[:-num_results-1:-1])\n",
    "    )\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa1e86-9138-433e-9d48-fef8c337d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(sorted_df, num_printing_results):\n",
    "    # print top n urls with similarity scores\n",
    "    print(\"\\nTop Similar Pages:\\n\" + \"=\"*40)\n",
    "    for i in range(min(num_printing_results, len(sorted_df))):\n",
    "        url = sorted_df.get_column(\"url\")[i]\n",
    "        score = sorted_df.get_column(\"similarity\")[i]\n",
    "        print(f\"{i+1}. {url.ljust(60)} | Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d579af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path):\n",
    "  pl.Config.set_float_precision(10)\n",
    "  # To give some more readiability, regardless of what is shown the numbers ARE the same!\n",
    "  df = pl.read_csv(file_path)\n",
    "  cleaned_df = df.with_columns(\n",
    "      pl.col(\"embedding\")\n",
    "        .str.strip_chars(\"[]\")\n",
    "        .str.replace_all(\"\\n\", '')\n",
    "        .str.strip_chars(\" \")\n",
    "        .str.split(' ')\n",
    "        .list.eval(pl.element().filter(pl.element() != \"\"))\n",
    "        .list.eval(pl.element().cast(pl.Float32, strict=False))\n",
    "  )\n",
    "  return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62700309-3308-4ff8-b9f5-cdafb6b01c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_council(\n",
    "    council_names,\n",
    "    max_depth=6,\n",
    "    keyword_scorer=None,\n",
    "    filters=None,\n",
    "    prompt=\"A page about conservation areas.\",\n",
    "    cache_enabled=False,\n",
    "    num_results=None,\n",
    "    num_printing_results=10,\n",
    "    save_dfs = False,\n",
    "    load_dfs = False,\n",
    "    data_dir = '',\n",
    "    chunking = False,\n",
    "    chunk_size = 300, \n",
    "    chunk_overlap = 50, \n",
    "    separators = [\"##\", \"\\n\\n\", \". \", \" \", \"\"], \n",
    "    keep_separator = False,\n",
    "):\n",
    "    crawler = Crawler(\n",
    "            max_depth=max_depth,\n",
    "            keyword_scorer=keyword_scorer,\n",
    "            filters=filters,\n",
    "            cache_enabled=cache_enabled,\n",
    "        )\n",
    "    \n",
    "    crawled_dfs = dict()\n",
    "\n",
    "    for council_name in council_names:\n",
    "        council_data = data.filter(pl.col(\"name\").str.contains(council_name))\n",
    "        short_council_name = council_data.get_column(\"website\")[0].split('.')[1]\n",
    "        full_name = council_data.get_column(\"name\")[0]\n",
    "        homepage = council_data.get_column(\"website\")[0]\n",
    "        prompt = prompt.format((full_name).replace('\\n', ''))\n",
    "        save_path = f\"{data_dir}{short_council_name}.csv\"\n",
    "\n",
    "        print(\"=\"*40 + f\"\\nProcessing {full_name}...\\n\")\n",
    "        \n",
    "        if os.path.isdir(data_dir) and load_dfs:\n",
    "            if f\"{short_council_name}.csv\" in os.listdir(data_dir): \n",
    "                sorted_df = load_csv_data(save_path)\n",
    "                pretty_print_results(sorted_df, num_printing_results)\n",
    "                crawled_dfs[homepage] = sorted_df\n",
    "                continue\n",
    "            \n",
    "        # crawl url\n",
    "        crawl_data = await crawler.deep_crawl(homepage)\n",
    "        \n",
    "        # get markdown embeddings\n",
    "        sorted_df = get_similar_urls(\n",
    "            crawl_data = crawl_data, \n",
    "            prompt = prompt,\n",
    "            num_results=num_results, \n",
    "            chunking=chunking,\n",
    "            chunk_size = chunk_size, \n",
    "            chunk_overlap = chunk_overlap, \n",
    "            separators = separators, \n",
    "            keep_separator = keep_separator\n",
    "        )\n",
    "        \n",
    "        sorted_df = sorted_df.with_columns(pl.col(\"url\").str.strip_chars_end(\"/\"))\n",
    "        \n",
    "        pretty_print_results(sorted_df, num_printing_results)\n",
    "\n",
    "        if save_dfs:\n",
    "            save_df = sorted_df.to_pandas()\n",
    "            save_df.to_csv(save_path)\n",
    "\n",
    "        crawled_dfs[homepage] = sorted_df\n",
    "    \n",
    "    return crawled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd082b60-59f0-4789-8051-2006e5e8f189",
   "metadata": {},
   "source": [
    "## Our approach\n",
    "\n",
    "Our approach involves 2 main steps: a web crawler and an embedding similarity search. Below is a description of these steps.\n",
    "\n",
    "### Web crawler\n",
    "\n",
    "The web crawler takes a homepage URL of an organisation (council website) and crawls it to look for pages talking about conservation areas.\n",
    "\n",
    "The crawler will look for links on a single page, put them in a queue and then iteratively check them until it finds what it was looking for or it reaches a stopping criterion, such as maximum depth (how many clicks away from home page). \n",
    "\n",
    "In order to save time, we can define some scorers or filters which tell the crawler which pages to prioritise or ignore. In this case, some common patterns of what a user needs to click to get to the page of interest are _\"planning\"_, _\"building\"_, _\"heritage\"_ or _\"conservation\"_.\n",
    "\n",
    "The crawler uses a *\"best first strategy\"*, which utilises the scorers or filters to visit most relevant sites first, rather than a depth-first or breath-first search.\n",
    "\n",
    "The crawler extracts the HTML from the pages and turns them into markdown. This is because it's more readable and easier to work with in the next steps. The crawler returns a list of pairs of (_url_, _markdown_).\n",
    "\n",
    "### Embedding search\n",
    "\n",
    "To be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81c3c2-abc0-409d-ac54-90c58848fd1a",
   "metadata": {},
   "source": [
    "### The next few cells show how to use the tools to find conservation area pages.\n",
    "\n",
    "You can define your own parameters, such as maximum depth, how many results you want to see and any scorers or filters. Below is a template showing how to defin each scorer/filter type correctly - all you need to do is change the keywords or patterns.\n",
    "\n",
    "You can also define a prompt - this is what will be used to get embeddings scores for a webpage. The more similar the prompt is to what a conservation area page usually looks like, the more accurate the results.\n",
    "\n",
    "Lastly, you can await the `process_council` function, which will run the functionality described above and print the results. You can use it for one council only or for a list of councils."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63761147-b9c6-46aa-8538-6d2f5c57bc4e",
   "metadata": {},
   "source": [
    "#### Template for how to define filters or scorers\n",
    "##### Pick the types you need and adjust their parameter (keywords, threshold, ...) and pass them to the crawler function.\n",
    "    keyword_scorer = {\n",
    "        \"keywords\": [\"conservation\", \"conservation area\", \"planning\", \"building\", \"urban\", \"heritage\", \"resident\"],\n",
    "        \"weight\": 0.8,\n",
    "    }\n",
    "        \n",
    "    filters=[\n",
    "        {\"type\": \"SEOFilter\", \"threshold\": 0.6, \"keywords\": [\"conservation\", \"area\", \"planning\", \"heritage\", \"resident\"]},\n",
    "        {\"type\": \"ContentRelevanceFilter\", \"query\": \"conservation area or planning data\", \"threshold\": 0.2},\n",
    "        {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "        {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48645d9-ab88-4369-bbb9-b76f282b9eeb",
   "metadata": {},
   "source": [
    "### Gedling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1c676-48fe-4a1a-b02f-5314d79fc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "gedling_dict = await process_council(\n",
    "    council_names=[\"Gedling\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f29b7a-f3ed-4d8c-8bf6-b59fa6bb9a4f",
   "metadata": {},
   "source": [
    "### South Gloucestershire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141c6a5-65bd-4d7d-8149-1b1a1491b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "sglos_dict = await process_council(\n",
    "    council_names=[\"South Gloucestershire\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results\n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a416094-8fbc-4283-8047-27d2e24bcc05",
   "metadata": {},
   "source": [
    "### Bournemouth, Christchurch and Poole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87c118-de61-4a7b-8b19-7d02ef474ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "bmccpl_dict = await process_council(\n",
    "    council_names=[\"Bournemouth, Christchurch and Poole\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec51f6-4058-489c-80d9-53c95de77a0c",
   "metadata": {},
   "source": [
    "### Warrington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614492c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "warr_dict = await process_council(\n",
    "    council_names=[\"Warrington\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16eca4-dff1-43a8-a28e-e847dc145dcc",
   "metadata": {},
   "source": [
    "### Stoke on Trent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a073d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "stoke_dict = await process_council(\n",
    "    council_names=[\"Stoke\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187695cb-e99c-40c6-9407-af0a7a3df0f7",
   "metadata": {},
   "source": [
    "### Redbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8348d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "redbridge_dict = await process_council(\n",
    "    council_names=[\"Redbridge\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286414dd-dd60-48f1-b09f-74d86be15a2c",
   "metadata": {},
   "source": [
    "### York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "york_dict = await process_council(\n",
    "    council_names=[\"York\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338095f7-217c-4afe-a2db-532759ffa411",
   "metadata": {},
   "source": [
    "### Malvern Hills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b106626",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "malvern_dict = await process_council(\n",
    "    council_names=[\"Malvern Hills\"], \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results, \n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a7493-5717-409f-843e-b98295bf5350",
   "metadata": {},
   "source": [
    "## Multiple councils\n",
    "\n",
    "You can define any list of councils and their processing will be executed sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab152b-5fac-480d-8538-d47b43247dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "num_examples = 10\n",
    "example_idx = np.random.randint(0, len(data), num_examples)\n",
    "examples = data[example_idx]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d402d-0ffb-46b7-98ef-2f6b813aaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "multi_dict = await process_council(\n",
    "    council_names=examples.get_column(\"name\"), \n",
    "    max_depth=max_depth, \n",
    "    filters=filters, \n",
    "    prompt=prompt, \n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b4c39",
   "metadata": {},
   "source": [
    "## Evaluating Results with Chunking\n",
    "\n",
    "For this section we manually labelled 25 councils with their true page. We perform tests and ranking here to assess the performance of our model. First we import a manually defined list of true documentation urls as test_df, and filter our main dataset for that. We also strip the final slash and will clean urls in other ways throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e25d7-db9d-4213-a3e0-a733ab14bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_csv(\"data/page_ranking_truth_df.csv\")\n",
    "test_df = test_df.with_columns(pl.col(\"correct_documentation_url\").str.strip_chars_end(\"/\"))\n",
    "test_website_list = test_df[\"website\"].to_list()\n",
    "data_filtered = data.filter(pl.col(\"website\").is_in(test_website_list))\n",
    "combined_df = data_filtered.join(test_df, on='website')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa45a98",
   "metadata": {},
   "source": [
    "The next cell parses all councils in our test set. This takes some time to run, but there is functionality for saving and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "Conservation Areas in {}\n",
    "\n",
    "We are committed to preserving the historic and architectural character of our borough\n",
    "through designated conservation areas. These areas protect buildings, streets, and \n",
    "landscapes of special significance, ensuring that any changes respect their unique heritage. \n",
    "If you live or own property in a conservation area, additional planning controls may \n",
    "apply to alterations, demolitions, and new developments. Our aim is to balance modern \n",
    "needs with the protection of our historic environment. For more information on conservation \n",
    "area guidelines, planning applications, and how you can contribute to local heritage preservation, \n",
    "please visit our planning and conservation pages. You will find maps, appraisal documents and the\n",
    "list of council conservation areas. \n",
    "\"\"\"\n",
    "# Fair warning, this will take multiple hours to run in its entirety. \n",
    "\n",
    "test_output_dict = await process_council(\n",
    "    council_names=combined_df.get_column(\"name\"), # Just the first 5 takes 20 mins.\n",
    "    max_depth=max_depth, \n",
    "    filters=filters,        \n",
    "    prompt=prompt, \n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    "    # save_dfs = True, # Uncomment if you want to save your parsed table. This will overwrite anything in the data_dir, so do change if needed.\n",
    "    # load_dfs = True, # Uncomment if you want to quickly load previous parses from data_dir. This will lead to no scraping being done.\n",
    "    # data_dir = 'data/chunking_csvs/', # Uncomment to specify save/load location.\n",
    "    chunking = True, # Set to False if you would like the entire webpage embedded, not chunks.\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba32036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_cleaning(df, col):\n",
    "    df = df.with_columns(pl.col(col).str.replace(\n",
    "        \"http://\", \"https://\", literal=True))\n",
    "    df = df.with_columns(pl.col(col).str.replace(\n",
    "        \"www.\", \"\", literal=True))\n",
    "    df = df.with_columns(pl.col(col).str.strip_chars_end(\"/\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ee831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    df = url_cleaning(df, 'url')\n",
    "    df = url_cleaning(df, 'id')\n",
    "\n",
    "    df = df.sort(\"similarity\", descending=True)\n",
    "    # We only want the best chunk (if chunking) from each webpage.\n",
    "    df = df.group_by(\"url\", maintain_order=True).first()\n",
    "    df = df.sort(by = 'similarity', descending=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_rank(correct_webpage, df):\n",
    "    # Here we get an ordered list of the similarity scores, along with the best one.\n",
    "    # Note, multiple rows may have the same similarity.\n",
    "    similarity_scores = list(set(df['similarity']))\n",
    "    sorted_similarity_scores = sorted(similarity_scores, reverse=True)\n",
    "    result_rows = df.filter(pl.col('url') == correct_webpage)\n",
    "    best_similarity_score = result_rows['similarity'][0]\n",
    "\n",
    "    # Now we acquire the rank of the embedding score achieved by our pipeline.\n",
    "    rank = sorted_similarity_scores.index(best_similarity_score) + 1\n",
    "    print(f'rank is {rank} with similarity {best_similarity_score}')\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e36ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking_processing(test_df: pl.dataframe.frame.DataFrame, test_output_dict: dict, outlier_threshold: int = 1000):\n",
    "    \"\"\"Processing to analyse the ranking of our pipeline's embedding guesses. \n",
    "\n",
    "    :param test_df: polars.DataFrame with columns:\n",
    "        - \"website\": The base website of the council\n",
    "        - \"correct_documentation_url\": The manually judged correct webpage. This should have the largest embedding similarity.\n",
    "    :param test_output_dict: dictionary with outputs of webscraping and embedding. Keys are base websites and values are dataframes.\n",
    "    :param outlier_threshold: Threshold for ranks beyond which results are assumed false/failure. If above, not saved in ranking_dict.\n",
    "    :return: \n",
    "        ranking_dict: Dictionary with ranks for each website. Base websites are keys, ranks are values\n",
    "        num_unclassified: Quantity of failed experiments, where no website match could be found.\n",
    "        num_outliers: Quantity of experiments leading to outliers.\n",
    "    \"\"\"\n",
    "    \n",
    "    ranking_dict = dict()\n",
    "    filtered_output_dict = {url: remove_duplicates(df)\n",
    "                        for url, df in test_output_dict.items()}\n",
    "    \n",
    "    num_unclassified = 0\n",
    "    num_outliers = 0\n",
    "\n",
    "    for base_website, similarity_df in filtered_output_dict.items():\n",
    "        print(f'processing {base_website}')\n",
    "        try:\n",
    "            correct_webpage = test_df.filter(pl.col('website') == base_website)['correct_documentation_url'][0]\n",
    "            rank = get_result_rank(correct_webpage, similarity_df)\n",
    "            if rank >= outlier_threshold:\n",
    "                print(f\"Outlier for this url - rank of {rank} - leaving out of plot\")\n",
    "                num_outliers += 1\n",
    "                continue\n",
    "            \n",
    "            ranking_dict[base_website] = rank\n",
    "        \n",
    "        except IndexError as e:\n",
    "            print(f\"No match for correct website via urls - error message '{e}'\")\n",
    "            num_unclassified += 1 \n",
    "                \n",
    "    return ranking_dict, num_unclassified, num_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b816033",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = url_cleaning(test_df, 'correct_documentation_url')\n",
    "ranking_dict, num_unclassified, num_outliers = test_ranking_processing(test_df, \n",
    "                                                                       test_output_dict, \n",
    "                                                                       outlier_threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list(ranking_dict.values())\n",
    "height = ranks.count(mode(ranks))\n",
    "\n",
    "ax = sns.histplot(x=ranks, binwidth=1, discrete=True, color='#00625E')\n",
    "ax.set_title(f'Frequency of Ranks - {num_unclassified} unclassified, {num_outliers} outlier/s')\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_ylabel('Quantity')\n",
    "ax.set_xticks(np.arange(1, max(ranks)+1))\n",
    "ax.set_yticks(np.arange(0, height+1, 2))\n",
    "ax.set_xticklabels([str(num) for num in np.arange(1, max(ranks)+1)])\n",
    "ax.set_yticklabels([str(num) for num in np.arange(0, height+1, 2)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98329f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(ranks):\n",
    "    size = len(ranks)\n",
    "    return sum([1/rank for rank in ranks])/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3232f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = mean_reciprocal_rank(ranks)\n",
    "print(f\"Mean Reciprocal Rank of - {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a7fb8",
   "metadata": {},
   "source": [
    "### Case Studies\n",
    "\n",
    "Let's look a little further at the ones that did not work out, starting with Camden. We see that the correct page is not scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_dict = {url: remove_duplicates(df)\n",
    "                        for url, df in test_output_dict.items()}\n",
    "\n",
    "correct_page = test_df.filter(\n",
    "    pl.col('website') == 'https://www.camden.gov.uk'\n",
    ")['correct_documentation_url'][0]\n",
    "\n",
    "print(filtered_output_dict[\n",
    "    'https://www.camden.gov.uk'\n",
    "].filter(pl.col(\"url\") == correct_page)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8747f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e6ab6",
   "metadata": {},
   "source": [
    "The page above (that is, one click away from the ideal one) is parsed, but the crawler struggles with security on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9eb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_above = 'https://camden.gov.uk/conservation-areas'\n",
    "print(filtered_output_dict[\n",
    "    'https://www.camden.gov.uk'\n",
    "].filter(pl.col(\"url\") == page_above)['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d3e63",
   "metadata": {},
   "source": [
    "Below is the code from the highest similarity chunk, taken from one of the boxes on https://camden.gov.uk/planning-building-development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_output_dict[\n",
    "    'https://www.camden.gov.uk'\n",
    "]['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee29c8",
   "metadata": {},
   "source": [
    "For Buckinghamshire, we see a few relevant links, the top 4 listed below. We assume https://buckinghamshire.gov.uk/planning-and-building-control/heritage/conservation-areas-in-buckinghamshire is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d145c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in filtered_output_dict['https://www.buckinghamshire.gov.uk']['url'][0,4,7,13]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27311e47",
   "metadata": {},
   "source": [
    "For West Berkshire, the top three are very close in terms of similarity rating. We assume https://westberks.gov.uk/conservationareas is the correct one, with the other two having highly relevant snippets of information on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in filtered_output_dict['https://www.westberks.gov.uk']['url', 'similarity'][:3].iter_rows():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c97d2f",
   "metadata": {},
   "source": [
    "For South Staffordshire, the true url (https://www.sstaffs.gov.uk/planning/conservation-and-heritage/south-staffordshires-conservation-areas) is very light on the detail usually present. The closest match (https://www.sstaffs.gov.uk/planning/conservation-and-heritage/conservation-areas) is one click away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in test_output_dict['https://www.sstaffs.gov.uk']['url'][0,7,11,30]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e056052",
   "metadata": {},
   "source": [
    "For Rochdale, the answer was not in the usual first 100 chunks. As such, here we run a full test keeping every single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca837ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_printing_results = 10\n",
    "filters=[\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\"type\": \"URLPatternFilter\", \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"]},\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "Conservation Areas in {}\n",
    "\n",
    "We are committed to preserving the historic and architectural character of our borough\n",
    "through designated conservation areas. These areas protect buildings, streets, and \n",
    "landscapes of special significance, ensuring that any changes respect their unique heritage. \n",
    "If you live or own property in a conservation area, additional planning controls may \n",
    "apply to alterations, demolitions, and new developments. Our aim is to balance modern \n",
    "needs with the protection of our historic environment. For more information on conservation \n",
    "area guidelines, planning applications, and how you can contribute to local heritage preservation, \n",
    "please visit our planning and conservation pages. You will find maps, appraisal documents and the\n",
    "list of council conservation areas. \n",
    "\"\"\"\n",
    "\n",
    "rochdale_dict = await process_council(\n",
    "    council_names=['Rochdale'],\n",
    "    max_depth=max_depth, \n",
    "    filters=filters,        \n",
    "    prompt=prompt, \n",
    "    num_printing_results=num_printing_results,\n",
    "    chunking = True,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6c89c",
   "metadata": {},
   "source": [
    "With this we see that the webpage was crawled, but something went wrong, presumably due to too much scraping being detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb96344",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rochdale_dict = {url: remove_duplicates(df)\n",
    "                        for url, df in rochdale_dict.items()}\n",
    "\n",
    "correct_page = test_df.filter(\n",
    "    pl.col('website') == 'https://www.rochdale.gov.uk'\n",
    ")['correct_documentation_url'][0]\n",
    "\n",
    "filtered_rochdale_dict[\n",
    "    'https://www.rochdale.gov.uk'\n",
    "].filter(pl.col('url') == correct_page)['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dffce",
   "metadata": {},
   "source": [
    "For Dover, the web scraper simply does not parse the website properly. It only crawls over two distinct pages, the home page and the planning page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_dict['https://www.dover.gov.uk']['url'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915746ac",
   "metadata": {},
   "source": [
    "This may be down to the .aspx extension used for the website, so the web crawler may not be appropriate in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushing_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
