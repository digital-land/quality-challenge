{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from statistics import mode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from data_quality_utils import Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# suppress warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from datasette\n",
    "datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "query = \"\"\"\n",
    "select *\n",
    "from source as s\n",
    "left join organisation as o\n",
    "on s.organisation=o.organisation\n",
    "where s.collection = \"conservation-area\"\n",
    "\"\"\"\n",
    "encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=(\"user\", \"pass\"))\n",
    "\n",
    "filename = \"datasette_data.csv\"\n",
    "with open(filename, \"wb\") as f_out:\n",
    "    f_out.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by organisation as we're looking for one page per council\n",
    "data = (\n",
    "    pl.read_csv(filename)\n",
    "    .group_by(\"name\")\n",
    "    .agg(pl.col(\"website\").first(), pl.col(\"documentation_url\"))\n",
    ")\n",
    "data = data.with_columns(pl.col(\"website\").str.strip_chars_end(\"/\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size, chunk_overlap, separators, keep_separator):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators,\n",
    "        keep_separator=keep_separator,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_processing(crawl_data, chunk_size, chunk_overlap, separators, keep_separator):\n",
    "    chunking_data = list()\n",
    "\n",
    "    for url, markdown in crawl_data:\n",
    "        chunk_texts = split_text(\n",
    "            markdown, chunk_size, chunk_overlap, separators, keep_separator\n",
    "        )\n",
    "        for chunk_num, chunk in enumerate(chunk_texts):\n",
    "            chunk_id = f\"{url}-{chunk_num}\"  # To have unique id for data.\n",
    "            chunking_data.append((chunk_id, url, chunk_num, chunk))\n",
    "\n",
    "    chunked_df = pl.DataFrame(\n",
    "        chunking_data, schema=[\"id\", \"url\", \"chunk_num\", \"text\"], orient=\"row\"\n",
    "    )\n",
    "\n",
    "    embeddings = embedding_model.encode(\n",
    "        chunked_df[\"text\"].to_list(), convert_to_numpy=True\n",
    "    )\n",
    "    chunked_df = chunked_df.with_columns(pl.Series(name=\"embedding\", values=embeddings))\n",
    "\n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_processing(crawl_data):\n",
    "    basic_data = list()\n",
    "\n",
    "    for url, markdown in crawl_data:\n",
    "        basic_data.append((f\"{url}-0\", url, 0, markdown))\n",
    "\n",
    "    basic_df = pl.DataFrame(\n",
    "        basic_data, schema=[\"id\", \"url\", \"chunk_num\", \"text\"], orient=\"row\"\n",
    "    )\n",
    "\n",
    "    embeddings = embedding_model.encode(\n",
    "        basic_df[\"text\"].to_list(), convert_to_numpy=True\n",
    "    )\n",
    "    basic_df = basic_df.with_columns(pl.Series(name=\"embedding\", values=embeddings))\n",
    "\n",
    "    return basic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_results(sorted_df, num_printing_results):\n",
    "    # print top n urls with similarity scores\n",
    "    print(\"\\nTop Similar Pages:\\n\" + \"=\" * 40)\n",
    "    for i in range(min(num_printing_results, len(sorted_df))):\n",
    "        url = sorted_df.get_column(\"url\")[i]\n",
    "        score = sorted_df.get_column(\"similarity\")[i]\n",
    "        print(f\"{i+1}. {url.ljust(60)} | Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_embeddings(\n",
    "    crawl_dfs,\n",
    "    prompt,\n",
    "    num_results=None,\n",
    "    num_printing_results=10,\n",
    "    chunking=False,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75,\n",
    "    separators=[\"##\", \"\\n\\n\", \". \", \" \", \"\"],\n",
    "    keep_separator=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Embeds crawled webpage data from , computes similarity to a given prompt, and returns the top N most similar pages.\n",
    "\n",
    "    Parameters:\n",
    "    - crawl_dfs (dictionary): A dictionary with key: value as url: crawl_df for each crawled site.\n",
    "    - prompt (str): The text prompt to compare against the crawled page embeddings.\n",
    "    - num_results (int, optional): The number of top similar pages to return. If None, returns all pages.\n",
    "    - num_printing_results (int, optional): The number of top similar pages to print.\n",
    "    - chunking (bool, optional): Whether we want to embed the full webpage or chunk.\n",
    "    - chunk_size (int, optional): Approximate size of our chunked text.\n",
    "    - chunk_overlap (int, optional): Amount of previous chunk to append/ovelap with new one.\n",
    "    - separators (list, optional): List of possible splitting points for our chunks.\n",
    "    - keep_separator (bool, optional): Whether we wish to keep the split string element in our chunked text.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - output_dict (dictionary): A dictionary with key: value as url: sorted_df for each crawled site.\n",
    "      Sorted_df is a DataFrame containing:\n",
    "        - \"id\": The id, a mix of url and chunk_num.\n",
    "        - \"url\": The webpage URL.\n",
    "        - \"chunk_num\": The index of the chunk, always 0 if chunking = False.\n",
    "        - \"text\": The extracted or chunked markdown content.\n",
    "        - \"embedding\": The computed embedding for the content.\n",
    "        - \"similarity\": The cosine similarity score with the prompt. This is how we sort the DataFrame.\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "\n",
    "    for homepage, df in crawl_dfs.items():\n",
    "\n",
    "        crawl_data = [(url, webpage) for url, webpage in df.iter_rows()]\n",
    "\n",
    "        if chunking:\n",
    "            crawl_df = chunk_processing(\n",
    "                crawl_data=crawl_data,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separators=separators,\n",
    "                keep_separator=keep_separator,\n",
    "            )\n",
    "        elif not chunking:\n",
    "            crawl_df = basic_processing(crawl_data)\n",
    "\n",
    "        short_council_name = homepage.split(\".\")[1]\n",
    "        full_name = data.filter(pl.col(\"website\").str.contains(short_council_name))[\n",
    "            \"name\"\n",
    "        ][0]\n",
    "        formatted_prompt = prompt.format((full_name).replace(\"\\n\", \"\"))\n",
    "        embeddings = np.stack(crawl_df[\"embedding\"].to_list())\n",
    "        prompt_embedding = np.array(\n",
    "            embedding_model.encode(formatted_prompt, convert_to_numpy=True),\n",
    "            dtype=\"float64\",\n",
    "        )\n",
    "\n",
    "        # get similarity scores\n",
    "        sim = util.cos_sim(\n",
    "            prompt_embedding.astype(np.float32), embeddings.astype(np.float32)\n",
    "        )\n",
    "\n",
    "        # get indices of top n most similar urls\n",
    "        if not num_results:\n",
    "            num_results = len(crawl_df)\n",
    "        indices = np.argsort(sim).numpy().flatten()[: -num_results - 1 : -1]\n",
    "        sorted_df = crawl_df[indices].with_columns(\n",
    "            similarity=np.sort(sim).flatten()[: -num_results - 1 : -1]\n",
    "        )\n",
    "        pretty_print_results(sorted_df, num_printing_results)\n",
    "        output_dict[homepage] = sorted_df\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_councils(\n",
    "    council_names,\n",
    "    max_depth=6,\n",
    "    keyword_scorer=None,\n",
    "    filters=None,\n",
    "    cache_enabled=False,\n",
    "    save_dfs=False,\n",
    "    load_dfs=False,\n",
    "    data_dir=\"\",\n",
    "):\n",
    "    crawler = Crawler(\n",
    "        max_depth=max_depth,\n",
    "        keyword_scorer=keyword_scorer,\n",
    "        filters=filters,\n",
    "        cache_enabled=cache_enabled,\n",
    "    )\n",
    "\n",
    "    crawled_dfs = dict()\n",
    "\n",
    "    for council_name in council_names:\n",
    "        df_data = list()\n",
    "        council_data = data.filter(pl.col(\"name\").str.contains(council_name))\n",
    "        short_council_name = council_data.get_column(\"website\")[0].split(\".\")[1]\n",
    "        full_name = council_data.get_column(\"name\")[0]\n",
    "        homepage = council_data.get_column(\"website\")[0]\n",
    "        save_path = f\"{data_dir}{short_council_name}.csv\"\n",
    "\n",
    "        print(\"=\" * 40 + f\"\\nProcessing {full_name}...\\n\")\n",
    "\n",
    "        if os.path.isdir(data_dir) and load_dfs:\n",
    "            if f\"{short_council_name}.csv\" in os.listdir(data_dir):\n",
    "                crawl_df = pl.read_csv(save_path)\n",
    "                crawled_dfs[homepage] = crawl_df\n",
    "                continue\n",
    "\n",
    "        # crawl url\n",
    "        crawl_data = await crawler.deep_crawl(homepage)\n",
    "\n",
    "        for url, markdown in crawl_data:\n",
    "            df_data.append((url, markdown))\n",
    "\n",
    "        crawl_df = pl.DataFrame(df_data, schema=[\"url\", \"text\"], orient=\"row\")\n",
    "\n",
    "        if save_dfs:\n",
    "            crawl_df.write_csv(save_path)\n",
    "\n",
    "        crawled_dfs[homepage] = crawl_df\n",
    "\n",
    "    return crawled_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Our approach\n",
    "\n",
    "Our approach involves 2 main steps: a web crawler and an embedding similarity search. Below is a description of these steps.\n",
    "\n",
    "### Web crawler\n",
    "\n",
    "The web crawler takes a homepage URL of an organisation (council website) and crawls it to look for pages talking about conservation areas.\n",
    "\n",
    "The crawler will look for links on a single page, put them in a queue and then iteratively check them until it finds what it was looking for or it reaches a stopping criterion, such as maximum depth (how many clicks away from home page). \n",
    "\n",
    "In order to save time, we can define some scorers or filters which tell the crawler which pages to prioritise or ignore. In this case, some common patterns of what a user needs to click to get to the page of interest are _\"planning\"_, _\"building\"_, _\"heritage\"_ or _\"conservation\"_.\n",
    "\n",
    "The crawler uses a *\"best first strategy\"*, which utilises the scorers or filters to visit most relevant sites first, rather than a depth-first or breath-first search.\n",
    "\n",
    "The crawler extracts the HTML from the pages and turns them into markdown. This is because it's more readable and easier to work with in the next steps. The crawler returns a list of pairs of (_url_, _markdown_).\n",
    "\n",
    "### Embedding search\n",
    "\n",
    "Embedding is a method where a vector representation of our scraped markdown text is generated, that we can use to measure similarity. The embedding model is trained to produce embeddings that preserve semantic meaning, that is, vectors that are 'close' have similar meanings. We measure this closeness through cosine similarity - essentially cosine of the angle difference between the two vectors in the range [-1,1], with 1 indicating a perfect match. \n",
    "\n",
    "Our goal is to find the webpage with the highest cosine similarity to our example prompt, which can be user specified for the specific task, here it identifies conservation areas but in principle could be changed for article 4 directions or similar.\n",
    "\n",
    "At present we have two strategies: \n",
    "1) Embed the entire webpage (default).\n",
    "2) Chunk the webpage into smaller texts (set `chunking = True`).\n",
    "\n",
    "If the latter is chosen, we trying to find the webpage <i>that <b>has</b> the chunk</i> with the highest embedding similarity. The parameter `chunk_size` determines the approximate size of these chunks, split at one of the `separators`. There is also `chunk_overlap` to specify how much of the previous chunk you want to begin the next, which is useful for preserving context.\n",
    "\n",
    "To limit the quantity of these matches returned, specify the cutoff, organised by similarity with `num_results`, and specify how many of these are printed after the crawler and embedding model has run with `num_printing_results`. These scores can be saved and loaded in `data_dir` using `save_dfs` and `load_dfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### The next few cells show how to use the tools to find conservation area pages.\n",
    "\n",
    "You can define your own parameters, such as maximum depth, how many results you want to see and any scorers or filters. Below is a template showing how to defin each scorer/filter type correctly - all you need to do is change the keywords or patterns.\n",
    "\n",
    "You can also define a prompt - this is what will be used to get embeddings scores for a webpage. The more similar the prompt is to what a conservation area page usually looks like, the more accurate the results.\n",
    "\n",
    "Lastly, you can await the `process_council` function, which will run the functionality described above and print the results. You can use it for one council only or for a list of councils."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Template for how to define filters or scorers\n",
    "##### Pick the types you need and adjust their parameter (keywords, threshold, ...) and pass them to the crawler function.\n",
    "    keyword_scorer = {\n",
    "        \"keywords\": [\"conservation\", \"conservation area\", \"planning\", \"building\", \"urban\", \"heritage\", \"resident\"],\n",
    "        \"weight\": 0.8,\n",
    "    }\n",
    "        \n",
    "    filters=[\n",
    "        {\"type\": \"SEOFilter\", \"threshold\": 0.6, \"keywords\": [\"conservation\", \"area\", \"planning\", \"heritage\", \"resident\"]},\n",
    "        {\"type\": \"ContentRelevanceFilter\", \"query\": \"conservation area or planning data\", \"threshold\": 0.2},\n",
    "        {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "        {\"type\": \"URLPatternFilter\", \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"]},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Gedling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "gedling_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Gedling\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "gedling_embedding_dict = get_dict_embeddings(\n",
    "    gedling_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### South Gloucestershire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "sglos_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"South Gloucestershire\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "sglos_embedding_dict = get_dict_embeddings(\n",
    "    sglos_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Bournemouth, Christchurch and Poole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "bmccpl_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Bournemouth, Christchurch and Poole\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "bmccpl_embedding_dict = get_dict_embeddings(\n",
    "    bmccpl_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Warrington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "warr_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Warrington\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "warr_embedding_dict = get_dict_embeddings(\n",
    "    warr_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Stoke on Trent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "stoke_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Stoke\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "stoke_embedding_dict = get_dict_embeddings(\n",
    "    stoke_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Redbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "redbridge_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Redbridge\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "redbridge_embedding_dict = get_dict_embeddings(\n",
    "    redbridge_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "york_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"York\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "york_embedding_dict = get_dict_embeddings(\n",
    "    york_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Malvern Hills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "malvern_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Malvern Hills\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "malvern_embedding_dict = get_dict_embeddings(\n",
    "    malvern_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Multiple councils\n",
    "\n",
    "You can define any list of councils and their processing will be executed sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "num_examples = 10\n",
    "example_idx = np.random.randint(0, len(data), num_examples)\n",
    "examples = data[example_idx]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [\"*conservation*\", \"*planning*\", \"*building*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "The text discusses conservation areas from the {} and includes data on \n",
    "planning data, areas, interactive maps, appraisals, notices, boundaries, \n",
    "links and similar.\n",
    "\"\"\"\n",
    "\n",
    "multi_scraped_dict = await scrape_councils(\n",
    "    council_names=examples.get_column(\"name\"),\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "multi_embedding_dict = get_dict_embeddings(\n",
    "    multi_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Evaluating Results with Chunking\n",
    "\n",
    "For this section we manually labelled 25 councils with their true page. We perform tests and ranking here to assess the performance of our model. First we import a manually defined list of true documentation urls as test_df, and filter our main dataset for that. We also strip the final slash and will clean urls in other ways throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_csv(\"data/page_ranking_truth_df.csv\")\n",
    "test_df = test_df.with_columns(\n",
    "    pl.col(\"correct_documentation_url\").str.strip_chars_end(\"/\")\n",
    ")\n",
    "data_filtered = data.filter(pl.col(\"website\").is_in(test_df[\"website\"]))\n",
    "combined_df = data_filtered.join(test_df, on=\"website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The next cell parses all councils in our test set. This takes some time to run, but there is functionality for saving and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_results = 100\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "Conservation Areas in {}\n",
    "\n",
    "We are committed to preserving the historic and architectural character of our borough\n",
    "through designated conservation areas. These areas protect buildings, streets, and \n",
    "landscapes of special significance, ensuring that any changes respect their unique heritage. \n",
    "If you live or own property in a conservation area, additional planning controls may \n",
    "apply to alterations, demolitions, and new developments. Our aim is to balance modern \n",
    "needs with the protection of our historic environment. For more information on conservation \n",
    "area guidelines, planning applications, and how you can contribute to local heritage preservation, \n",
    "please visit our planning and conservation pages. You will find maps, appraisal documents and the\n",
    "list of council conservation areas. \n",
    "\"\"\"\n",
    "# Fair warning, this will take multiple hours to run in its entirety.\n",
    "\n",
    "test_scraping_dict = await scrape_councils(\n",
    "    council_names=combined_df.get_column(\"name\"),\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    "    cache_enabled=False,\n",
    "    # save_dfs = True, # Uncomment if you want to save your parsed table. This will overwrite anything in the data_dir, so do change if needed.\n",
    "    # load_dfs = True, # Uncomment if you want to quickly load previous parses from data_dir. This will lead to no scraping being done.\n",
    "    data_dir=\"\",  # Uncomment to specify save/load location.\n",
    ")\n",
    "\n",
    "test_output_dict = get_dict_embeddings(\n",
    "    crawl_dfs=test_scraping_dict,\n",
    "    prompt=prompt,\n",
    "    num_results=num_results,\n",
    "    num_printing_results=num_printing_results,\n",
    "    chunking=True,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_cleaning(df, col):\n",
    "    df = df.with_columns(pl.col(col).str.replace(\"http://\", \"https://\", literal=True))\n",
    "    df = df.with_columns(pl.col(col).str.replace(\"www.\", \"\", literal=True))\n",
    "    df = df.with_columns(pl.col(col).str.strip_chars_end(\"/\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    df = url_cleaning(df, \"url\")\n",
    "    df = url_cleaning(df, \"id\")\n",
    "\n",
    "    df = df.sort(\"similarity\", descending=True)\n",
    "    # We only want the best chunk (if chunking) from each webpage.\n",
    "    df = df.group_by(\"url\", maintain_order=True).first()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_rank(correct_webpage: str, df: pl.dataframe.frame.DataFrame) -> int:\n",
    "    \"\"\"Obtains the rank of the correct_webpage sorted by embedding similarity score. If chunking, this returns the rank of the highest similarity chunk on the correct_webpage.\n",
    "\n",
    "    :param correct_webpage: url of the judged correct webpage for the test.\n",
    "    :param df: A DataFrame containing:\n",
    "        - \"url\": The webpage URL.\n",
    "        - \"markdown\": The extracted markdown content.\n",
    "        - \"embedding\": The computed embedding for the content.\n",
    "        - \"similarity\": The cosine similarity score with the prompt.\n",
    "    :return:\n",
    "        rank: Rank of correct website in search.\n",
    "    \"\"\"\n",
    "    similarity_scores = list(set(df[\"similarity\"]))\n",
    "    sorted_similarity_scores = sorted(similarity_scores, reverse=True)\n",
    "    result_rows = df.filter(pl.col(\"url\") == correct_webpage)\n",
    "    best_similarity_score = result_rows[\"similarity\"][0]\n",
    "\n",
    "    # Now we acquire the rank of the embedding score achieved by our pipeline.\n",
    "    rank = sorted_similarity_scores.index(best_similarity_score) + 1\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking_processing(\n",
    "    test_df: pl.dataframe.frame.DataFrame,\n",
    "    test_output_dict: dict,\n",
    "    outlier_threshold: int = 1000,\n",
    "):\n",
    "    \"\"\"Processing to analyse the ranking of our pipeline's embedding guesses.\n",
    "\n",
    "    :param test_df: polars.DataFrame with columns:\n",
    "        - \"website\": The base website of the council\n",
    "        - \"correct_documentation_url\": The manually judged correct webpage. This should have the largest embedding similarity.\n",
    "    :param test_output_dict: dictionary with outputs of webscraping and embedding. Keys are base websites and values are dataframes.\n",
    "    :param outlier_threshold: Threshold for ranks beyond which results are assumed false/failure. If above, not saved in ranking_dict.\n",
    "    :return:\n",
    "        ranking_dict: Dictionary with ranks for each website. Base websites are keys, ranks are values\n",
    "        num_unclassified: Quantity of failed experiments, where no website match could be found.\n",
    "        num_outliers: Quantity of experiments leading to outliers.\n",
    "    \"\"\"\n",
    "\n",
    "    ranking_dict = dict()\n",
    "    filtered_output_dict = {\n",
    "        url: remove_duplicates(df) for url, df in test_output_dict.items()\n",
    "    }\n",
    "\n",
    "    num_unclassified = 0\n",
    "    num_outliers = 0\n",
    "\n",
    "    for base_website, similarity_df in filtered_output_dict.items():\n",
    "        print(f\"processing {base_website}\")\n",
    "        try:\n",
    "            correct_webpage = test_df.filter(pl.col(\"website\") == base_website)[\n",
    "                \"correct_documentation_url\"\n",
    "            ][0]\n",
    "            rank = get_result_rank(correct_webpage, similarity_df)\n",
    "            if rank >= outlier_threshold:\n",
    "                print(f\"Outlier for this url - rank of {rank} - leaving out of plot\")\n",
    "                num_outliers += 1\n",
    "                continue\n",
    "\n",
    "            ranking_dict[base_website] = rank\n",
    "\n",
    "        except IndexError as e:\n",
    "            print(f\"No match for correct website via urls - error message '{e}'\")\n",
    "            num_unclassified += 1\n",
    "\n",
    "    return ranking_dict, num_unclassified, num_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = url_cleaning(test_df, \"correct_documentation_url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dict, num_unclassified, num_outliers = test_ranking_processing(\n",
    "    test_df, test_output_dict, outlier_threshold=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list(ranking_dict.values())\n",
    "height = ranks.count(mode(ranks))\n",
    "\n",
    "ax = sns.histplot(x=ranks, binwidth=1, discrete=True, color=\"#00625E\")\n",
    "ax.set_title(\n",
    "    f\"Frequency of Ranks - {num_unclassified} unclassified, {num_outliers} outlier/s\"\n",
    ")\n",
    "ax.set_xlabel(\"Rank\")\n",
    "ax.set_ylabel(\"Quantity\")\n",
    "ax.set_xticks(np.arange(1, max(ranks) + 1))\n",
    "ax.set_yticks(np.arange(0, height + 1, 2))\n",
    "ax.set_xticklabels([str(num) for num in np.arange(1, max(ranks) + 1)])\n",
    "ax.set_yticklabels([str(num) for num in np.arange(0, height + 1, 2)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(ranks):\n",
    "    size = len(ranks)\n",
    "    return sum([1 / rank for rank in ranks]) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = mean_reciprocal_rank(ranks)\n",
    "print(f\"Mean Reciprocal Rank of - {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Case Studies\n",
    "\n",
    "Let's look a little further at the ones that did not work out, starting with Camden. We see that the correct page is not scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_dict = {\n",
    "    url: remove_duplicates(df) for url, df in test_output_dict.items()\n",
    "}\n",
    "\n",
    "correct_page = test_df.filter(pl.col(\"website\") == \"https://www.camden.gov.uk\")[\n",
    "    \"correct_documentation_url\"\n",
    "][0]\n",
    "\n",
    "print(\n",
    "    filtered_output_dict[\"https://www.camden.gov.uk\"].filter(\n",
    "        pl.col(\"url\") == correct_page\n",
    "    )[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The page above (that is, one click away from the ideal one) is parsed, but the crawler struggles with security on the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_above = \"https://camden.gov.uk/conservation-areas\"\n",
    "print(\n",
    "    filtered_output_dict[\"https://www.camden.gov.uk\"].filter(\n",
    "        pl.col(\"url\") == page_above\n",
    "    )[\"text\"][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Below is the code from the highest similarity chunk, taken from one of the boxes on https://camden.gov.uk/planning-building-development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_output_dict[\"https://www.camden.gov.uk\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "For Rochdale, the answer was not in the usual first 100 chunks. As such, here we run a full test keeping every single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "num_printing_results = 10\n",
    "filters = [\n",
    "    {\"type\": \"ContentTypeFilter\", \"allowed_types\": [\"text/html\"]},\n",
    "    {\n",
    "        \"type\": \"URLPatternFilter\",\n",
    "        \"patterns\": [r\"*[Cc]onservation*\", r\"*[Pp]lanning*\", r\"*[Bb]uilding*\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# please write the prompt such that there is a curly bracket where the council\n",
    "# name will be inserted\n",
    "prompt = \"\"\"\n",
    "Conservation Areas in {}\n",
    "\n",
    "We are committed to preserving the historic and architectural character of our borough\n",
    "through designated conservation areas. These areas protect buildings, streets, and \n",
    "landscapes of special significance, ensuring that any changes respect their unique heritage. \n",
    "If you live or own property in a conservation area, additional planning controls may \n",
    "apply to alterations, demolitions, and new developments. Our aim is to balance modern \n",
    "needs with the protection of our historic environment. For more information on conservation \n",
    "area guidelines, planning applications, and how you can contribute to local heritage preservation, \n",
    "please visit our planning and conservation pages. You will find maps, appraisal documents and the\n",
    "list of council conservation areas. \n",
    "\"\"\"\n",
    "\n",
    "rochdale_scraped_dict = await scrape_councils(\n",
    "    council_names=[\"Rochdale\"],\n",
    "    max_depth=max_depth,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "# Now we have a dictionary object with a key as the homepage of the website, and a value of the dataframe\n",
    "# This can then be used to add on our embedding scores.\n",
    "\n",
    "rochdale_embedding_dict = get_dict_embeddings(\n",
    "    rochdale_scraped_dict,\n",
    "    prompt=prompt,\n",
    "    num_printing_results=num_printing_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "With this we see that the webpage was crawled, but something went wrong, presumably due to too much scraping being detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rochdale_dict = {\n",
    "    url: remove_duplicates(df) for url, df in rochdale_embedding_dict.items()\n",
    "}\n",
    "\n",
    "correct_page = test_df.filter(pl.col(\"website\") == \"https://www.rochdale.gov.uk\")[\n",
    "    \"correct_documentation_url\"\n",
    "][0]\n",
    "\n",
    "filtered_rochdale_dict[\"https://www.rochdale.gov.uk\"].filter(\n",
    "    pl.col(\"url\") == correct_page\n",
    ")[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "For Dover, the web scraper simply does not parse the website properly. It only crawls over two distinct pages, the home page and the planning page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_dict[\"https://www.dover.gov.uk\"][\"url\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "This may be down to the .aspx extension used for the website, so the web crawler may not be appropriate in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quality-challenge-OS-ME5O1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
