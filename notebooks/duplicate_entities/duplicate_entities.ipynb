{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Duplicate Entities\n",
    "\n",
    "This notebook is intended to test methods for detecting duplicate entities in planning data. It has been developed by focusing on conservation areas but should be easily extended to other entity types as it uses standard properties of the entities such as name and geometry.\n",
    "\n",
    "The goal is to flag specific issues and then prioritise entities based on the number of issues  that are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import requests\n",
    "from shapely import MultiPolygon, overlaps\n",
    "from shapely.wkt import loads\n",
    "\n",
    "from data_quality_utils.polygon.plotting import plot_multipolygon\n",
    "from data_quality_utils.polygon.utils import overlap_ratio, shortest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duplicates(duplicate_entities: list[MultiPolygon]):\n",
    "    fig = go.Figure()\n",
    "    for i, polygon in enumerate(duplicate_entities):\n",
    "        fig = plot_multipolygon(\n",
    "            polygon=polygon,\n",
    "            fig=fig,\n",
    "            name=f\"{i}\",\n",
    "            line_color=\"black\",\n",
    "            fill_color=(255, 0, 0),\n",
    "            fill_alpha=0.3,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        geo_scope=\"europe\",\n",
    "        map=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lon=polygon.centroid.x, lat=polygon.centroid.y),\n",
    "            zoom=13,\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        margin={\"r\": 100, \"t\": 50, \"l\": 100, \"b\": 50},\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "From our search of `datasette`, there appears to be no publicly available endpoint for obtaining all entities in the planning data. Instead, we use the `conservation-area` dataset to get all conservation areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservation_entities_url = \"https://datasette.planning.data.gov.uk/conservation-area/entity.csv?_stream=on&_size=max\"\n",
    "r = requests.get(conservation_entities_url, auth=(\"user\", \"pass\"))\n",
    "conservation_df = pl.read_csv(r.content)\n",
    "conservation_df = conservation_df.with_columns(\n",
    "    json=conservation_df[\"json\"].str.json_decode(infer_schema_length=10000)\n",
    ").unnest(\"json\")\n",
    "\n",
    "conservation_df = conservation_df.filter(pl.col(\"name\") != \"Polygon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Duplicate Checks\n",
    "\n",
    "We do this in two parts. First we identify all potential duplicates and then we rank them by defining 'issues' and counting the number of issue each set of potential duplicates has. Issues in this context are facts about the entities that would be unusual if they are not duplicates - eg. a large amount of overlap between their geometries. \n",
    "\n",
    "### Identifying Candidates\n",
    "The simplest check is whether two or more entities in the same region share the same name. We group the data by these columns and keep all entities that have two or more rows per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = (\n",
    "    conservation_df.group_by([\"name\", \"organisation_entity\"])\n",
    "    .len()\n",
    "    .rename({\"len\": \"count\"})\n",
    ")\n",
    "candidate_df = count_df.filter(count_df[\"count\"] > 1).sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(candidate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = conservation_df.group_by([\"name\"]).len().rename({\"len\": \"count\"})\n",
    "candidate_df = count_df.filter(count_df[\"count\"] > 1).sort(\"count\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Counting Issues\n",
    "\n",
    "If we demand candidate duplicates share a name, we can then identify issues that may indicate that two entities are duplicates. \n",
    "\n",
    "We'll look for missing information, for fewer unique values like organisation name than one would expect for the number of entities with the same name, and missing or overlapping geometries. The general method here is that by counting the number of issues that are present for each candidate set of duplicates, we can prioritise them for investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_distance(pairs):\n",
    "    distances = []\n",
    "    for a, b in pairs:\n",
    "        if a and b:\n",
    "            distances.append(shortest_distance(a, b))\n",
    "    if distances:\n",
    "        return min(distances)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def max_overlap(pairs):\n",
    "    areas = []\n",
    "    for a, b in pairs:\n",
    "        if a and b:\n",
    "            areas.append(overlap_ratio(a, b))\n",
    "    if areas:\n",
    "        return max(areas)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def count_real_orgs(organisations):\n",
    "    real_orgs = organisations.filter(~organisations.is_in([16, 600001]))\n",
    "    return real_orgs.n_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "First generate some useful metrics that we expect are related to whether two or more entities with the same name are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for name in candidate_df[\"name\"]:\n",
    "\n",
    "    filtered_entities_df = conservation_df.filter(pl.col(\"name\") == name)\n",
    "\n",
    "    possible_duplicates: list[MultiPolygon] = [\n",
    "        loads(shape_str) for shape_str in filtered_entities_df[\"geometry\"].to_list()\n",
    "    ]\n",
    "    candidate_pairs = list(combinations(possible_duplicates, 2))\n",
    "\n",
    "    metrics = {\n",
    "        \"Name\": name,\n",
    "        \"Number of candidates\": len(possible_duplicates),\n",
    "        \"Number of organisations\": count_real_orgs(\n",
    "            filtered_entities_df[\"organisation_entity\"]\n",
    "        ),\n",
    "        \"Number of documentation-urls\": filtered_entities_df[\"documentation-url\"]\n",
    "        .drop_nans()\n",
    "        .n_unique(),\n",
    "        \"Number of document-urls\": filtered_entities_df[\"document-url\"]\n",
    "        .drop_nans()\n",
    "        .n_unique(),\n",
    "        \"Missing Geometries\": filtered_entities_df[\"geometry\"].is_null().sum(),\n",
    "        \"Max Overlap\": max_overlap(candidate_pairs),\n",
    "        \"Number of Overlaps\": sum([overlaps(a, b) for a, b in candidate_pairs]),\n",
    "        \"Smallest Distance\": min_distance(candidate_pairs),\n",
    "    }\n",
    "    records.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Then go through and count the number of metrics that have values that indicate a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_groups_df = pl.DataFrame(records)\n",
    "candidate_groups_df = candidate_groups_df.with_columns(\n",
    "    issue_count=(\n",
    "        # fewer unique documents than candidates\n",
    "        (\n",
    "            candidate_groups_df[\"Number of document-urls\"]\n",
    "            < candidate_groups_df[\"Number of candidates\"]\n",
    "        )\n",
    "        # fewer unique organisations than candidates\n",
    "        + (\n",
    "            candidate_groups_df[\"Number of candidates\"]\n",
    "            > candidate_groups_df[\"Number of organisations\"]\n",
    "        )\n",
    "        # fewer unique documentation pages than candidates\n",
    "        + (\n",
    "            candidate_groups_df[\"Number of documentation-urls\"]\n",
    "            < candidate_groups_df[\"Number of candidates\"]\n",
    "        )\n",
    "        # candidates have at least some overlap in geography\n",
    "        + (candidate_groups_df[\"Max Overlap\"].fill_null(0.0) > 0)\n",
    "        # Candidates have more than 1 overlapping polygon\n",
    "        + (candidate_groups_df[\"Number of Overlaps\"] > 1)\n",
    "        # The smallest distance between candidats is less than a km\n",
    "        + (candidate_groups_df[\"Smallest Distance\"].fill_null(100) < 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_groups_df.filter(pl.col(\"issue_count\") > 3).sort(\n",
    "    by=\"issue_count\", descending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Overview\n",
    "\n",
    "- We find 986 groups of entities with the same name\n",
    "- 685 of these have at least one missing geometry, 58 have no geometries at all.\n",
    "- Assuming at lease one issue would be present in any duplicate group, there are 506 groups that truly are duplicates.\n",
    "- High issue counts do seem to point to clear duplicates\n",
    "- Overlap can be used to find obvious duplicates\n",
    "- Missing/incomplete data is a major problem that should be resolved before entity de-duplication.\n",
    "\n",
    "\n",
    "## Case Studies\n",
    "\n",
    "### Fragmented Entities\n",
    "Wymondham Conservation Area is typical of the high issue count candidates. There are 13 entities that share a name, documentation-url and document-url. However, there are 13 unique geometries in a set of 13 \"duplicates\". These 13 areas form one contiguous area which is documented just once on the South Norfolk District Council website. Wymondham is specifically listed as a single conservation area on their website so it seems this is a single entity where the polygons that define it have been logged separately.\n",
    "\n",
    "Several other top listed candidates (Diss, Wingham, Petersfield) have the same issue though Petersfield has the additional issue of one entity that covers the whole area plus 8 entities that record each of its sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Wymondham Conservation Area\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\") == name)[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Complete Overlap\n",
    "\n",
    "Sorting instead by the amount of overlap between polygons, we get entities that are clear duplicates. For conservation areas like Stone, we get a small number of duplicate entities with essentially identical geometries. For more complex cases like Farringdon, we get one overarching polygon with several other entities that completely duplicate sections of it, leading to high overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Stone\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\") == name)[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Another common failure mode is when two entities have the same name but should be under different organiations. For example Poulton which appears to be the name of a place in the Cotswolds as well as Cheshire but both have been assigned to MHCLG (entity 600001) instead of their local council. \n",
    "\n",
    "These cases are also often missing geometries but do have documentation-urls. If some of the previous validation steps were run documentation-url and document-url might be fixed. A simple rule to validate the organisation given the documentation-url might fix the assignment to MHCLG. Geometry might be obtained if the organisation leads to the correct endpoint or if AI tools are used to extract polygons from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Poulton\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Tiverton\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Moor Park\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhclg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
