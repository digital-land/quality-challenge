{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Duplicate Entities\n",
    "\n",
    "This notebook is intended to test methods for detecting duplicate entities in planning data. It has been developed by focusing on conservation areas but should be easily extended to other entity types as it uses standard properties of the entities such as name and geometry.\n",
    "\n",
    "The goal is to flag specific issues and then prioritise entities based on the number of issues  that are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import requests\n",
    "from rtree import index\n",
    "from shapely import MultiPolygon, overlaps\n",
    "from shapely.wkt import loads\n",
    "\n",
    "from data_quality_utils.polygon.plotting import plot_multipolygon\n",
    "from data_quality_utils.polygon.utils import overlap_ratio, shortest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duplicates(duplicate_entities: list[MultiPolygon], fill_colours=None, zoom=15):\n",
    "    fig = go.Figure()\n",
    "    if fill_colours is None:\n",
    "        fill_colours = [(255, 0, 0)] * len(duplicate_entities)\n",
    "    for i, polygon in enumerate(duplicate_entities):\n",
    "        fig = plot_multipolygon(\n",
    "            polygon=polygon,\n",
    "            fig=fig,\n",
    "            name=f\"{i+1}\",\n",
    "            line_color=\"black\",\n",
    "            fill_color=fill_colours[i],\n",
    "            fill_alpha=0.3,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        geo_scope=\"europe\",\n",
    "        map=dict(\n",
    "            style=\"open-street-map\",\n",
    "            center=dict(lon=polygon.centroid.x, lat=polygon.centroid.y),\n",
    "            zoom=zoom,\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        margin={\"r\": 100, \"t\": 50, \"l\": 100, \"b\": 50},\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Union-Find (also known as Disjoint Set Union or DSU) is a data structure used to efficiently group items into disjoint sets and check whether two items belong to the same group. It supports two key operations:\n",
    "\n",
    "- find(x) — returns the root representative of the set containing x\n",
    "- union(x, y) — merges the sets containing x and y\n",
    "\n",
    "In grouping candidates Union-Find helps identify clusters of spatially nearby entities by merging entities whose geometries fall within a given distance (e.g. 100 meters). As entities are iterated over and found to be close to others, they’re merged into the same group. This enables fast grouping of related entities without redundant or nested checks, ensuring all spatially connected entities end up in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent.get(x, x) != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent.get(x, x)\n",
    "\n",
    "    def union(self, x, y):\n",
    "        self.parent[self.find(x)] = self.find(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "From our search of `datasette`, there appears to be no publicly available endpoint for obtaining all entities in the planning data. Instead, we use the `conservation-area` dataset to get all conservation areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get organisation names\n",
    "datasette_base_url = \"https://datasette.planning.data.gov.uk/digital-land.csv\"\n",
    "\n",
    "query = \"\"\"\n",
    "select *\n",
    "from organisation as o\n",
    "\"\"\"\n",
    "encoded_query = urllib.parse.urlencode({\"sql\": query})\n",
    "\n",
    "r = requests.get(f\"{datasette_base_url}?{encoded_query}\", auth=(\"user\", \"pass\"))\n",
    "\n",
    "org_filename = \"org_data.csv\"\n",
    "with open(org_filename, \"wb\") as f_out:\n",
    "    f_out.write(r.content)\n",
    "\n",
    "org = (\n",
    "    pl.read_csv(org_filename)\n",
    "    .select([\"entity\", \"name\"])\n",
    "    .rename({\"entity\": \"organisation_entity\", \"name\": \"organisation_name\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservation_entities_url = \"https://datasette.planning.data.gov.uk/conservation-area/entity.csv?_stream=on&_size=max\"\n",
    "r = requests.get(conservation_entities_url, auth=(\"user\", \"pass\"))\n",
    "conservation_df = pl.read_csv(r.content)\n",
    "conservation_df = (\n",
    "    conservation_df.with_columns(\n",
    "        json=conservation_df[\"json\"].str.json_decode(infer_schema_length=10000)\n",
    "    )\n",
    "    .unnest(\"json\")\n",
    "    .join(org, on=\"organisation_entity\")\n",
    ")\n",
    "\n",
    "conservation_df = conservation_df.filter(pl.col(\"name\") != \"Polygon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Duplicate Checks\n",
    "\n",
    "We do this in two parts. First we identify all potential duplicates and then we rank them by defining 'issues' and counting the number of issue each set of potential duplicates has. Issues in this context are facts about the entities that would be unusual if they are not duplicates - eg. a large amount of overlap between their geometries. \n",
    "\n",
    "### Identifying Candidates\n",
    "The simplest check is whether two or more entities in the same region share the same name. We group the data by these columns and keep all entities that have two or more rows per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_candidate_df = (\n",
    "    conservation_df.group_by(\"name\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .filter(pl.col(\"count\") > 1)\n",
    "    .with_row_index(\"group_id\")\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "candidate_df = conservation_df.join(\n",
    "    name_candidate_df, on=\"name\", how=\"left\"\n",
    ").with_columns(pl.col(\"group_id\").fill_null(-1).cast(pl.Int32))\n",
    "candidate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Another way is to create groups based on the proximity of their polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick entities with non-null geometries\n",
    "geometry_df = candidate_df.filter(pl.col(\"geometry\").is_not_null()).select(\n",
    "    [\"entity\", \"geometry\", \"group_id\"]\n",
    ")\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    geometry_df.to_dict(as_series=False),\n",
    "    geometry=[loads(wkt) for wkt in geometry_df[\"geometry\"].to_list()],\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(\"EPSG:27700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = gdf[\"entity\"].tolist()\n",
    "geoms = gdf[\"geometry\"].tolist()\n",
    "group_ids = gdf[\"group_id\"].to_list()\n",
    "\n",
    "uf = UnionFind()\n",
    "\n",
    "# create a spatial index for efficient search for nearby geometries\n",
    "spatial_index = index.Index()\n",
    "for idx, geom in enumerate(geoms):\n",
    "    if geom and not geom.is_empty:\n",
    "        spatial_index.insert(idx, geom.bounds)\n",
    "\n",
    "# search for entities within 1000 metres\n",
    "bbox_expand_meters = 100\n",
    "\n",
    "for i, geom1 in enumerate(geoms):\n",
    "    if not geom1 or geom1.is_empty:\n",
    "        continue\n",
    "\n",
    "    minx, miny, maxx, maxy = geom1.bounds\n",
    "    expanded_bbox = (\n",
    "        minx - bbox_expand_meters,\n",
    "        miny - bbox_expand_meters,\n",
    "        maxx + bbox_expand_meters,\n",
    "        maxy + bbox_expand_meters,\n",
    "    )\n",
    "\n",
    "    nearby = list(spatial_index.intersection(expanded_bbox))\n",
    "    for j in nearby:\n",
    "        if i != j and geoms[j].distance(geom1) < bbox_expand_meters:\n",
    "            uf.union(i, j)\n",
    "\n",
    "# create clusters of entities based on name and proximity\n",
    "clusters = defaultdict(list)\n",
    "for i in range(len(entities)):\n",
    "    root = uf.find(i)\n",
    "    clusters[root].append(i)\n",
    "\n",
    "next_group_id = max(group_ids) + 1 if group_ids else 0\n",
    "cluster_id_map = {}\n",
    "\n",
    "# assign group IDs\n",
    "for root, members in clusters.items():\n",
    "    existing_ids = [group_ids[i] for i in members if group_ids[i] >= 0]\n",
    "    if existing_ids:\n",
    "        assigned_id = min(existing_ids)\n",
    "    else:\n",
    "        assigned_id = next_group_id\n",
    "        next_group_id += 1\n",
    "    for i in members:\n",
    "        cluster_id_map[i] = assigned_id\n",
    "\n",
    "final_group_ids = [cluster_id_map.get(i, group_ids[i]) for i in range(len(entities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_candidate_df = pl.DataFrame({\"entity\": entities, \"group_id\": final_group_ids})\n",
    "full_candidate_df = (\n",
    "    candidate_df.join(\n",
    "        geometry_candidate_df.rename({\"group_id\": \"group_id_spatial\"}),\n",
    "        on=\"entity\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.when(\n",
    "            pl.col(\"group_id_spatial\").is_not_null() & (pl.col(\"group_id_spatial\") >= 0)\n",
    "        )\n",
    "        .then(pl.col(\"group_id_spatial\"))\n",
    "        .otherwise(pl.col(\"group_id\"))\n",
    "        .alias(\"group_id\")\n",
    "    )\n",
    "    .drop([\"group_id_spatial\"])\n",
    "    .filter(pl.col(\"group_id\") >= 0)\n",
    ")\n",
    "valid_group_ids = (\n",
    "    full_candidate_df.group_by(\"group_id\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .filter(pl.col(\"count\") >= 2)\n",
    ")\n",
    "full_candidate_df = full_candidate_df.join(valid_group_ids, on=\"group_id\")\n",
    "full_candidate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Counting Issues\n",
    "\n",
    "If we demand candidate duplicates share a name, we can then identify issues that may indicate that two entities are duplicates. \n",
    "\n",
    "We'll look for missing information, for fewer unique values like organisation name than one would expect for the number of entities with the same name, and missing or overlapping geometries. The general method here is that by counting the number of issues that are present for each candidate set of duplicates, we can prioritise them for investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_distance(pairs):\n",
    "    distances = []\n",
    "    for a, b in pairs:\n",
    "        if a and b:\n",
    "            distances.append(shortest_distance(a, b))\n",
    "    if distances:\n",
    "        return min(distances)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def max_overlap(pairs):\n",
    "    areas = []\n",
    "    for a, b in pairs:\n",
    "        if a and b:\n",
    "            areas.append(overlap_ratio(a, b))\n",
    "    if areas:\n",
    "        return max(areas)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def count_real_orgs(organisations):\n",
    "    real_orgs = organisations.filter(~organisations.is_in([16, 600001]))\n",
    "    return real_orgs.n_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "First generate some useful metrics that we expect are related to whether two or more entities with the same name are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for group_id, filtered_entities_df in full_candidate_df.group_by(\"group_id\"):\n",
    "\n",
    "    filtered_names = filtered_entities_df.get_column(\"name\").to_list()\n",
    "\n",
    "    possible_duplicates: list[MultiPolygon] = [\n",
    "        loads(shape_str) for shape_str in filtered_entities_df[\"geometry\"].to_list()\n",
    "    ]\n",
    "    candidate_pairs = list(combinations(possible_duplicates, 2))\n",
    "\n",
    "    metrics = {\n",
    "        \"Group ID\": group_id,\n",
    "        \"Names\": filtered_names,\n",
    "        \"Number of names\": len(filtered_names),\n",
    "        \"Number of candidates\": len(possible_duplicates),\n",
    "        \"Number of organisations\": count_real_orgs(\n",
    "            filtered_entities_df[\"organisation_entity\"]\n",
    "        ),\n",
    "        \"Number of documentation-urls\": filtered_entities_df[\"documentation-url\"]\n",
    "        .drop_nans()\n",
    "        .n_unique(),\n",
    "        \"Number of document-urls\": filtered_entities_df[\"document-url\"]\n",
    "        .drop_nans()\n",
    "        .n_unique(),\n",
    "        \"Missing Geometries\": filtered_entities_df[\"geometry\"].is_null().sum(),\n",
    "        \"Max Overlap\": max_overlap(candidate_pairs),\n",
    "        \"Number of Overlaps\": sum([overlaps(a, b) for a, b in candidate_pairs]),\n",
    "        \"Smallest Distance\": min_distance(candidate_pairs),\n",
    "    }\n",
    "    records.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Then go through and count the number of metrics that have values that indicate a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_groups_df = pl.DataFrame(records)\n",
    "candidate_groups_df = candidate_groups_df.with_columns(\n",
    "    issue_count=(\n",
    "        # fewer unique documents than candidates\n",
    "        (\n",
    "            candidate_groups_df[\"Number of document-urls\"]\n",
    "            < candidate_groups_df[\"Number of candidates\"]\n",
    "        )\n",
    "        # fewer unique organisations than candidates\n",
    "        + (\n",
    "            candidate_groups_df[\"Number of candidates\"]\n",
    "            > candidate_groups_df[\"Number of organisations\"]\n",
    "        )\n",
    "        # fewer unique documentation pages than candidates\n",
    "        + (\n",
    "            candidate_groups_df[\"Number of documentation-urls\"]\n",
    "            < candidate_groups_df[\"Number of candidates\"]\n",
    "        )\n",
    "        # candidates have at least some overlap in geography\n",
    "        + (candidate_groups_df[\"Max Overlap\"].fill_null(0.0) > 0)\n",
    "        # Candidates have more than 1 overlapping polygon\n",
    "        + (candidate_groups_df[\"Number of Overlaps\"] > 1)\n",
    "        # The smallest distance between candidats is less than a km\n",
    "        + (candidate_groups_df[\"Smallest Distance\"].fill_null(100) < 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    candidate_groups_df.filter(\n",
    "        (pl.col(\"issue_count\") > 0) & (pl.col(\"Number of candidates\") <= 10)\n",
    "    ).sort(by=\"issue_count\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Overview\n",
    "\n",
    "- We find 986 groups of entities with the same name, 976 have at least one issue\n",
    "- We find additional 381 groups based on proximity\n",
    "- Out of 1365 total groups there are 1214 with at least one issue\n",
    "- High issue counts do seem to point to clear duplicates\n",
    "- Overlap can be used to find obvious duplicates\n",
    "- Missing/incomplete data is a major problem that should be resolved before entity de-duplication\n",
    "- Grouping by proximity can include distinct entities in the same group, but it also finds clear duplicates with slight name variations\n",
    "\n",
    "\n",
    "## Case Studies\n",
    "\n",
    "### Fragmented Entities\n",
    "Wymondham Conservation Area is typical of the high issue count candidates. There are 13 entities that share a name, documentation-url and document-url. However, there are 13 unique geometries in a set of 13 \"duplicates\". These 13 areas form one contiguous area which is documented just once on the South Norfolk District Council website. Wymondham is specifically listed as a single conservation area on their website so it seems this is a single entity where the polygons that define it have been logged separately.\n",
    "\n",
    "Several other top listed candidates (Diss, Wingham, Petersfield) have the same issue though Petersfield has the additional issue of one entity that covers the whole area plus 8 entities that record each of its sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Wymondham Conservation Area\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\") == name)[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "colours = [\n",
    "    (np.random.randint(0, 255), 0, np.random.randint(0, 255))\n",
    "    for i in range(len(possible_duplicates))\n",
    "]\n",
    "fig = plot_duplicates(possible_duplicates, fill_colours=colours, zoom=13)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Complete Overlap\n",
    "\n",
    "Sorting instead by the amount of overlap between polygons, we get entities that are clear duplicates. For conservation areas like Stone, we get a small number of duplicate entities with essentially identical geometries. For more complex cases like Farringdon, we get one overarching polygon with several other entities that completely duplicate sections of it, leading to high overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Stone\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\") == name)[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "colours = [(120, 120, 120), (255, 0, 0), (0, 0, 255)]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates, fill_colours=colours, zoom=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Entities with name variations\n",
    "\n",
    "Using only name to identify candidate groups can miss out on entities with very close/overlapping polygons but slightly different names. An example is Pulham Market, which has 3 entities listed under \"Pulham Market Conservation Area\" belonging to South Norfolk District Council, and one under \"Pulham Market\" belonging to Historic England."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    conservation_df.filter(pl.col(\"name\").str.contains(\"Pulham Market\"))\n",
    "    .group_by(\"organisation_name\")\n",
    "    .agg(pl.col(\"name\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Pulham Market\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\").str.contains(name))[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "colours = [\n",
    "    (np.random.randint(0, 255), 0, np.random.randint(0, 255))\n",
    "    for i in range(len(possible_duplicates))\n",
    "]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates, fill_colours=colours, zoom=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Entities with incorrect names\n",
    "\n",
    "Some entities are not listed under their correct names and instead use code names or other identifiers. In the case of Abinger Hammer we have 3 overlapping polygons, one covering the whole area and 2 smaller ones covering parts. However the two with correct names are listed under Historic England whereas the unnamed one correctly belongs to Mole Valley District Council. Identifying candidates only by name would not flag this case, but using polygon distance both\n",
    "- identifies duplicates,\n",
    "- resolves names and\n",
    "- resolves organisation entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = [44002548, 44004253, 44008695]\n",
    "(\n",
    "    conservation_df.filter(pl.col(\"entity\").is_in(entity_ids))\n",
    "    .group_by(\"organisation_name\")\n",
    "    .agg(pl.col(\"name\"), pl.col(\"entity\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = [44002548, 44004253, 44008695]\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"entity\").is_in(entity_ids))[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "colours = [\n",
    "    (np.random.randint(0, 255), 0, np.random.randint(0, 255))\n",
    "    for i in range(len(possible_duplicates))\n",
    "]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates, fill_colours=colours, zoom=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Entities with name variation, under different organisations and with missing geometries\n",
    "\n",
    "The duplicate candidate finder identified a group of 5 entities listed under different names: Salhouse, Salhouse Conservation Area, Salhouse BA Conservation Area, listed under Broadland District Council, MHCLG or Historic England. Their polygons are completely overlapping which makes them a clear duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Salhouse\"\n",
    "(\n",
    "    conservation_df.filter(pl.col(\"name\").str.contains(name))\n",
    "    .group_by(\"organisation_name\")\n",
    "    .agg(pl.col(\"name\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Salhouse\"\n",
    "possible_duplicates: list[MultiPolygon] = [\n",
    "    loads(shape_str)\n",
    "    for shape_str in conservation_df.filter(pl.col(\"name\").str.contains(name))[\n",
    "        \"geometry\"\n",
    "    ].to_list()\n",
    "    if shape_str\n",
    "]\n",
    "\n",
    "colours = [\n",
    "    (np.random.randint(0, 255), 0, np.random.randint(0, 255))\n",
    "    for i in range(len(possible_duplicates))\n",
    "]\n",
    "\n",
    "fig = plot_duplicates(possible_duplicates, fill_colours=colours, zoom=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Another common failure mode is when two entities have the same name but should be under different organiations. For example Poulton which appears to be the name of a place in the Cotswolds as well as Cheshire but both have been assigned to MHCLG (entity 600001) instead of their local council. \n",
    "\n",
    "These cases are also often missing geometries but do have documentation-urls. If some of the previous validation steps were run documentation-url and document-url might be fixed. A simple rule to validate the organisation given the documentation-url might fix the assignment to MHCLG. Geometry might be obtained if the organisation leads to the correct endpoint or if AI tools are used to extract polygons from documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Poulton\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Tiverton\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Moor Park\"\n",
    "conservation_df.filter(pl.col(\"name\") == name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quality-challenge-7gqK3ydY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
